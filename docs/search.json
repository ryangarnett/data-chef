[
  {
    "objectID": "recipes/import-csv-linux.html",
    "href": "recipes/import-csv-linux.html",
    "title": "Import CSV File Linux",
    "section": "",
    "text": "Description\n      \n    \n  \n  \n    Basic method for reading a csv file on a Linux computer\n\n  \n  \n  \n\n\n\n\n\n\n\n\n  \n    \n    \n  \n  \n    \n      Ingredients\n    \n    \n    \n      Package\n      Data\n    \n  \n  \n    readr\n\nsample_dwelling_characteristics.csv\n\n  \n  \n  \n\n\n\n\n\nSample Instructions\n\npackage::function(\"~/file_path/file_name.csv\")\n\n\nActual Instructions\n\nreadr::read_csv(\"~/data/sample_dwelling_characteristics.csv\")\n\n\nOutput\n\n\n\n\n\n\n  \n    \n      municipal_unit\n      assessment_account_number\n      civic_number\n      civic_additional\n      civic_direction\n      civic_street_name\n      civic_street_suffix\n      civic_city_name\n      living_units\n      year_built\n      square_foot_living_area\n      style\n      bedrooms\n      bathrooms\n      under_construction\n      construction_grade\n      finished_basement\n      garage\n      y_map_coordinate\n      x_map_coordinate\n      map_coordinates\n    \n  \n  \n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n10703689\nNA\nNA\nNA\nALPS\nRD\nPORTERS LAKE\n1\n2016\n1226\n1 Storey\n2\n2\nN\nAverage\nN\nN\nNA\nNA\nNA\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n08566615\n10\nNA\nNA\nSNEEZY\nAVE\nLAKE ECHO\n1\n1973\n768\nManufactured Home\n2\n1\nN\nManufactured Home\nN\nN\nNA\nNA\nNA\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n09781315\n108\nNA\nNA\nATIKIAN\nDR\nEASTERN PASSAGE\n1\n2008\n1400\n2 Storey\n3\n3\nN\nAverage\nN\nN\n44.61654\n-63.4618\n(44.61654, -63.4618)\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n08567875\n19\nNA\nNA\nTHE OTHER\nST\nPORTERS LAKE\n1\n1991\n896\nManufactured Home\n2\n1\nN\nManufactured Home\nN\nN\nNA\nNA\nNA\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n10535001\n15\nNA\nNA\nKINGS WHARF\nPL\nDARTMOUTH\n1\n2012\nNA\nNA\n2\n2\nN\nNA\nN\nN\nNA\nNA\nNA\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n10738822\n25\nNA\nNA\nALDERNEY\nDR\nDARTMOUTH\n1\n2016\nNA\nNA\n2\n2\nN\nNA\nN\nN\nNA\nNA\nNA"
  },
  {
    "objectID": "recipes/create-character-vector.html",
    "href": "recipes/create-character-vector.html",
    "title": "Create a Character Vector",
    "section": "",
    "text": "Description\n      \n    \n  \n  \n    Create a vector object in memory that has multiple character values\n\n  \n  \n  \n\n\n\n\n\n\n\n\n  \n    \n    \n  \n  \n    \n      Ingredients\n    \n    \n    \n      Package\n      Data\n    \n  \n  \n    NA\nNA\n  \n  \n  \n\n\n\n\n\nSample Instructions\n\nobject_name <- c(\"text\", \"text\", \"text\")\n\n\nActual Instructions\n\ncharacter_vector <- c(\"one\", \"two\", \"three\")\n\n\nOutput\n\n\n[1] \"one\"   \"two\"   \"three\""
  },
  {
    "objectID": "recipes/import-xlsx.html",
    "href": "recipes/import-xlsx.html",
    "title": "Import Excel File",
    "section": "",
    "text": "Description\n      \n    \n  \n  \n    Basic method for reading an excel file\n\n  \n  \n  \n\n\n\n\n\n\n\n\n  \n    \n    \n  \n  \n    \n      Ingredients\n    \n    \n    \n      Package\n      Data\n    \n  \n  \n    readxl\n\nsample_housing_data.xlsx\n\n  \n  \n  \n\n\n\n\n\nSample Instructions\n\npackage::function(\"file_path/file_name.xlsx\")\n\n\nActual Instructions\n\nreadxl::read_excel(\"C:/data/sample.xlsx\")\n\n\nOutput\n\n\n\n\n\n\n  \n    \n      municipal_unit\n      assessment_account_number\n      civic_number\n      civic_additional\n      civic_direction\n      civic_street_name\n      civic_street_suffix\n      civic_city_name\n      tax_year\n      assessed_value\n      taxable_assessed_value\n      y_map_coordinate\n      x_map_coordinate\n      map_coordinates\n    \n  \n  \n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n4610415\n40\nNA\nNA\nJUNIPER\nCRES\nEASTERN PASSAGE\n2018\n57100\n42700\nNA\nNA\nNA\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n10535174\n15\nNA\nNA\nKINGS WHARF\nPL\nDARTMOUTH\n2020\n282700\n282700\nNA\nNA\nNA\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n822639\n40\nNA\nNA\nCAMPBELL\nAVE\nDARTMOUTH\n2020\n26600\n16800\nNA\nNA\nNA\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n9037063\n275\nNA\nNA\nWINDMILL\nRD\nDARTMOUTH\n2020\n72500\n72500\nNA\nNA\nNA\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n10535301\n15\nNA\nNA\nKINGS WHARF\nPL\nDARTMOUTH\n2018\n283200\n283200\nNA\nNA\nNA\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n9282262\n352\nNA\nNA\nBROOKS\nDR\nEAST PRESTON\n2020\n16000\n16000\n44.72913\n-63.42245\n(44.72913, -63.42245)"
  },
  {
    "objectID": "recipes/create-numeric-variable.html",
    "href": "recipes/create-numeric-variable.html",
    "title": "Create a Numeric Variable",
    "section": "",
    "text": "Description\n      \n    \n  \n  \n    Create a variable object in memory for a specific number value\n\n  \n  \n  \n\n\n\n\n\n\n\n\n  \n    \n    \n  \n  \n    \n      Ingredients\n    \n    \n    \n      Package\n      Data\n    \n  \n  \n    NA\nNA\n  \n  \n  \n\n\n\n\n\nSample Instructions\n\nobject_name <- number\n\n\nActual Instructions\n\ncharacter_variable <- 10"
  },
  {
    "objectID": "recipes/load-multiple-packages.html",
    "href": "recipes/load-multiple-packages.html",
    "title": "Load Multiple Packages",
    "section": "",
    "text": "Description\n      \n    \n  \n  \n    Method to load and gain access to the functionality of multiple packages in R\n\n  \n  \n  \n\n\n\n\n\n\n\n\n  \n    \n    \n  \n  \n    \n      Ingredients\n    \n    \n    \n      Package\n      Data\n    \n  \n  \n    NA\nNA\n  \n  \n  \n\n\n\n\n\nSample Instructions\n\nlibrary(package_name1)\nlibrary(package_name2)\nlibrary(package_name3)\n\n\nActual Instructions\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyr)"
  },
  {
    "objectID": "recipes/install-multiple-packages.html",
    "href": "recipes/install-multiple-packages.html",
    "title": "Install Multiple Packages",
    "section": "",
    "text": "Description\n      \n    \n  \n  \n    Method to install multiple packages in R\n\n  \n  \n  \n\n\n\n\n\n\n\n\n  \n    \n    \n  \n  \n    \n      Ingredients\n    \n    \n    \n      Package\n      Data\n    \n  \n  \n    NA\nNA\n  \n  \n  \n\n\n\n\n\nSample Instructions\n\ninstall.packages(c(\"package_name1\",\"package_name2\", \"package_name3\"))\n\n\nActual Instructions\n\ninstall.packages(c(\"readr\", \"dplyr\", \"tidyr\"))"
  },
  {
    "objectID": "recipes/install-single-package.html",
    "href": "recipes/install-single-package.html",
    "title": "Install a Single Package",
    "section": "",
    "text": "Description\n      \n    \n  \n  \n    Method to install a single package in R\n\n  \n  \n  \n\n\n\n\n\n\n\n\n  \n    \n    \n  \n  \n    \n      Ingredients\n    \n    \n    \n      Package\n      Data\n    \n  \n  \n    NA\nNA\n  \n  \n  \n\n\n\n\n\nSample Instructions\n\ninstall.packages(\"package_name\")\n\n\nActual Instructions\n\ninstall.packages(\"readr\")"
  },
  {
    "objectID": "recipes/load-single-package.html",
    "href": "recipes/load-single-package.html",
    "title": "Load Single Package",
    "section": "",
    "text": "Description\n      \n    \n  \n  \n    Method to load and gain access to the functionality of a package in R\n\n  \n  \n  \n\n\n\n\n\n\n\n\n  \n    \n    \n  \n  \n    \n      Ingredients\n    \n    \n    \n      Package\n      Data\n    \n  \n  \n    NA\nNA\n  \n  \n  \n\n\n\n\n\nSample Instructions\n\nlibrary(package_name)\n\n\nActual Instructions\n\nlibrary(readr)"
  },
  {
    "objectID": "recipes/create-dataframe.html",
    "href": "recipes/create-dataframe.html",
    "title": "Create a Data Frame",
    "section": "",
    "text": "Description\n      \n    \n  \n  \n    Create a dataframe object in memory that has multiple character and numeric columns\n\n  \n  \n  \n\n\n\n\n\n\n\n\n  \n    \n    \n  \n  \n    \n      Ingredients\n    \n    \n    \n      Package\n      Data\n    \n  \n  \n    NA\nNA\n  \n  \n  \n\n\n\n\n\nSample Instructions\n\ndataframe_name <- data.frame(first_column_name = c(number, number, number, number),\n                             second_column_name = c(\"text\", \"text\", \"text\", \"text\"),\n                             third_column_name = c(number, number, number, number),\n                             forth_column_name = c(number, number, number, number)\n                             )\n\n\nActual Instructions\n\natlantic_capital_temperatures <- data.frame(station_id = c(100, 101, 102, 103),\n                                            province = c(\"NL\", \"NS\", \"PE\", \"NB\"),\n                                            city = c(\"St John's\", \"Halifax\", \"Charlottetown\", \"Saint John\"),\n                                            temperature = c(10, 15.1, 6.3, -4.3)\n                                            )\n\n\nOutput\n\n\n\n\n\n\n  \n    \n      station_id\n      province\n      city\n      temperature\n    \n  \n  \n    100\nNL\nSt John's\n10.0\n    101\nNS\nHalifax\n15.1\n    102\nPE\nCharlottetown\n6.3\n    103\nNB\nSaint John\n-4.3"
  },
  {
    "objectID": "recipes/create-variable-equation.html",
    "href": "recipes/create-variable-equation.html",
    "title": "Create Variable from an Equation",
    "section": "",
    "text": "Description\n      \n    \n  \n  \n    Create a variable object in memory for a specific mathematical equation\n\n  \n  \n  \n\n\n\n\n\n\n\n\n  \n    \n    \n  \n  \n    \n      Ingredients\n    \n    \n    \n      Package\n      Data\n    \n  \n  \n    NA\nNA\n  \n  \n  \n\n\n\n\n\nSample Instructions\n\nobject_name <- number + number\n\n\nActual Instructions\n\ntotal_variable <- 10 + 10"
  },
  {
    "objectID": "recipes/create-combining-variable.html",
    "href": "recipes/create-combining-variable.html",
    "title": "Create Variable from Combining Variables",
    "section": "",
    "text": "Description\n      \n    \n  \n  \n    Create a variable object in memory by combining two variable objects together\n\n  \n  \n  \n\n\n\n\n\n\n\n\n  \n    \n    \n  \n  \n    \n      Ingredients\n    \n    \n    \n      Package\n      Data\n    \n  \n  \n    NA\nNA\n  \n  \n  \n\n\n\n\n\nPreparation\n\nfirst_number_variable <- 5\nsecond_number_variable <- 10\n\n\nSample Instructions\n\nobject_name <- variable1 + variable2\n\n\nActual Instructions\n\ncombined_variable <- first_number_variable + second_number_variable"
  },
  {
    "objectID": "recipes/import-csv-repos.html",
    "href": "recipes/import-csv-repos.html",
    "title": "Import CSV File from a Repository",
    "section": "",
    "text": "Description\n      \n    \n  \n  \n    Basic method for reading a csv file stored in a cloud repository\n\n  \n  \n  \n\n\n\n\n\n\n\n\n  \n    \n    \n  \n  \n    \n      Ingredients\n    \n    \n    \n      Package\n      Data\n    \n  \n  \n    readr\n\nsample_dwelling_characteristics.csv\n\n  \n  \n  \n\n\n\n\n\nSample Instructions\n\npackage::function(\"url/file_name.csv\")\n\n\nActual Instructions\n\nreadr::read_csv(\"https://codeberg.org/data-chef/data/raw/branch/main/sample_dwelling_characteristics.csv\")\n\n\nOutput\n\n\n\n\n\n\n  \n    \n      municipal_unit\n      assessment_account_number\n      civic_number\n      civic_additional\n      civic_direction\n      civic_street_name\n      civic_street_suffix\n      civic_city_name\n      living_units\n      year_built\n      square_foot_living_area\n      style\n      bedrooms\n      bathrooms\n      under_construction\n      construction_grade\n      finished_basement\n      garage\n      y_map_coordinate\n      x_map_coordinate\n      map_coordinates\n    \n  \n  \n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n10703689\nNA\nNA\nNA\nALPS\nRD\nPORTERS LAKE\n1\n2016\n1226\n1 Storey\n2\n2\nN\nAverage\nN\nN\nNA\nNA\nNA\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n08566615\n10\nNA\nNA\nSNEEZY\nAVE\nLAKE ECHO\n1\n1973\n768\nManufactured Home\n2\n1\nN\nManufactured Home\nN\nN\nNA\nNA\nNA\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n09781315\n108\nNA\nNA\nATIKIAN\nDR\nEASTERN PASSAGE\n1\n2008\n1400\n2 Storey\n3\n3\nN\nAverage\nN\nN\n44.61654\n-63.4618\n(44.61654, -63.4618)\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n08567875\n19\nNA\nNA\nTHE OTHER\nST\nPORTERS LAKE\n1\n1991\n896\nManufactured Home\n2\n1\nN\nManufactured Home\nN\nN\nNA\nNA\nNA\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n10535001\n15\nNA\nNA\nKINGS WHARF\nPL\nDARTMOUTH\n1\n2012\nNA\nNA\n2\n2\nN\nNA\nN\nN\nNA\nNA\nNA\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n10738822\n25\nNA\nNA\nALDERNEY\nDR\nDARTMOUTH\n1\n2016\nNA\nNA\n2\n2\nN\nNA\nN\nN\nNA\nNA\nNA"
  },
  {
    "objectID": "recipes/import-xlsx-number-rows.html",
    "href": "recipes/import-xlsx-number-rows.html",
    "title": "Read number of rows on excel import",
    "section": "",
    "text": "Description\n      \n    \n  \n  \n    Read a specific number of rows when reading an excel file\n\n  \n  \n  \n\n\n\n\n\n\n\n\n  \n    \n    \n  \n  \n    \n      Ingredients\n    \n    \n    \n      Package\n      Data\n    \n  \n  \n    readxl\n\nsample_housing_data.xlsx\n\n  \n  \n  \n\n\n\n\n\nSample Instructions\n\npackage::function(\"file_path/file_name.xlsx\", n_max = number)\n\n\nActual Instructions\n\nreadxl::read_excel(\"C:/data/sample.xlsx\", n_max = 5)\n\n\nOutput\n\n\n\n\n\n\n  \n    \n      municipal_unit\n      assessment_account_number\n      civic_number\n      civic_additional\n      civic_direction\n      civic_street_name\n      civic_street_suffix\n      civic_city_name\n      tax_year\n      assessed_value\n      taxable_assessed_value\n      y_map_coordinate\n      x_map_coordinate\n      map_coordinates\n    \n  \n  \n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n4610415\n40\nNA\nNA\nJUNIPER\nCRES\nEASTERN PASSAGE\n2018\n57100\n42700\nNA\nNA\nNA\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n10535174\n15\nNA\nNA\nKINGS WHARF\nPL\nDARTMOUTH\n2020\n282700\n282700\nNA\nNA\nNA\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n822639\n40\nNA\nNA\nCAMPBELL\nAVE\nDARTMOUTH\n2020\n26600\n16800\nNA\nNA\nNA\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n9037063\n275\nNA\nNA\nWINDMILL\nRD\nDARTMOUTH\n2020\n72500\n72500\nNA\nNA\nNA\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n10535301\n15\nNA\nNA\nKINGS WHARF\nPL\nDARTMOUTH\n2018\n283200\n283200\nNA\nNA\nNA"
  },
  {
    "objectID": "recipes/import-csv-number_rows.html",
    "href": "recipes/import-csv-number_rows.html",
    "title": "Read Number of Rows on CSV Import",
    "section": "",
    "text": "Description\n      \n    \n  \n  \n    Read a specific number of rows when reading a csv file\n\n  \n  \n  \n\n\n\n\n\n\n\n\n  \n    \n    \n  \n  \n    \n      Ingredients\n    \n    \n    \n      Package\n      Data\n    \n  \n  \n    readr\n\nsample_dwelling_characteristics.csv\n\n  \n  \n  \n\n\n\n\n\nSample Instructions\n\npackage::function(\"file_path/file_name.csv\", n_max = number)\n\n\nActual Instructions\n\nreadr::read_csv(\"C:/data/sample_dwelling_characteristics.csv\", n_max = 6)\n\n\nOutput\n\n\n\n\n\n\n  \n    \n      municipal_unit\n      assessment_account_number\n      civic_number\n      civic_additional\n      civic_direction\n      civic_street_name\n      civic_street_suffix\n      civic_city_name\n      living_units\n      year_built\n      square_foot_living_area\n      style\n      bedrooms\n      bathrooms\n      under_construction\n      construction_grade\n      finished_basement\n      garage\n      y_map_coordinate\n      x_map_coordinate\n      map_coordinates\n    \n  \n  \n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n10703689\nNA\nNA\nNA\nALPS\nRD\nPORTERS LAKE\n1\n2016\n1226\n1 Storey\n2\n2\nN\nAverage\nN\nN\nNA\nNA\nNA\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n08566615\n10\nNA\nNA\nSNEEZY\nAVE\nLAKE ECHO\n1\n1973\n768\nManufactured Home\n2\n1\nN\nManufactured Home\nN\nN\nNA\nNA\nNA\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n09781315\n108\nNA\nNA\nATIKIAN\nDR\nEASTERN PASSAGE\n1\n2008\n1400\n2 Storey\n3\n3\nN\nAverage\nN\nN\n44.61654\n-63.4618\n(44.61654, -63.4618)\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n08567875\n19\nNA\nNA\nTHE OTHER\nST\nPORTERS LAKE\n1\n1991\n896\nManufactured Home\n2\n1\nN\nManufactured Home\nN\nN\nNA\nNA\nNA\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n10535001\n15\nNA\nNA\nKINGS WHARF\nPL\nDARTMOUTH\n1\n2012\nNA\nNA\n2\n2\nN\nNA\nN\nN\nNA\nNA\nNA\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n10738822\n25\nNA\nNA\nALDERNEY\nDR\nDARTMOUTH\n1\n2016\nNA\nNA\n2\n2\nN\nNA\nN\nN\nNA\nNA\nNA"
  },
  {
    "objectID": "recipes/import-xlsx-skip-rows.html",
    "href": "recipes/import-xlsx-skip-rows.html",
    "title": "Skip Rows on Excel Import",
    "section": "",
    "text": "Description\n      \n    \n  \n  \n    Skip rows when reading excel files\n\n  \n  \n  \n\n\n\n\n\n\n\n\n  \n    \n    \n  \n  \n    \n      Ingredients\n    \n    \n    \n      Package\n      Data\n    \n  \n  \n    readxl\n\nsample_housing_data.xlsx\n\n  \n  \n  \n\n\n\n\n\nSample Instructions\n\npackage::function(\"file_path/file_name.xlsx\", skip = number)\n\n\nActual Instructions\n\nreadxl::read_excel(\"C:/data/sample.xlsx\", skip = 2)\n\n\nOutput\n\n\n\n\n\n\n  \n    \n      HALIFAX REGIONAL MUNICIPALITY (HRM)\n      10535174\n      15\n      NA...4\n      NA...5\n      KINGS WHARF\n      PL\n      DARTMOUTH\n      2020\n      282700...10\n      282700...11\n      NA...12\n      NA...13\n      NA...14\n    \n  \n  \n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n822639\n40\nNA\nNA\nCAMPBELL\nAVE\nDARTMOUTH\n2020\n26600\n16800\nNA\nNA\nNA\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n9037063\n275\nNA\nNA\nWINDMILL\nRD\nDARTMOUTH\n2020\n72500\n72500\nNA\nNA\nNA\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n10535301\n15\nNA\nNA\nKINGS WHARF\nPL\nDARTMOUTH\n2018\n283200\n283200\nNA\nNA\nNA\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n9282262\n352\nNA\nNA\nBROOKS\nDR\nEAST PRESTON\n2020\n16000\n16000\n44.72913\n-63.42245\n(44.72913, -63.42245)\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n9911871\n56\nNA\nNA\nLINDENWOOD\nTERR\nDARTMOUTH\n2020\n485700\n467200\n44.6611\n-63.52719\n(44.6611, -63.52719)\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n10642906\n67\nNA\nNA\nKINGS WHARF\nPL\nDARTMOUTH\n2018\n602400\n602400\nNA\nNA\nNA"
  },
  {
    "objectID": "recipes/import-xlsx-specific-sheet.html",
    "href": "recipes/import-xlsx-specific-sheet.html",
    "title": "Read Specific Sheet on Excel Import",
    "section": "",
    "text": "Description\n      \n    \n  \n  \n    Read a specific sheet of an excel file\n\n  \n  \n  \n\n\n\n\n\n\n\n\n  \n    \n    \n  \n  \n    \n      Ingredients\n    \n    \n    \n      Package\n      Data\n    \n  \n  \n    readxl\n\nsample_housing_data.xlsx\n\n  \n  \n  \n\n\n\n\n\nSample Instructions\n\npackage::function(\"file_path/file_name.xlsx\", sheet = \"sheet_name\")\n\n\nActual Instructions\n\nreadxl::read_excel(\"C:/data/sample_housing_data.xlsx\", sheet = \"sample_dwelling_characteristics\")\n\n\nOutput\n\n\n\n\n\n\n  \n    \n      municipal_unit\n      assessment_account_number\n      civic_number\n      civic_additional\n      civic_direction\n      civic_street_name\n      civic_street_suffix\n      civic_city_name\n      living_units\n      year_built\n      square_foot_living_area\n      style\n      bedrooms\n      bathrooms\n      under_construction\n      construction_grade\n      finished_basement\n      garage\n      y_map_coordinate\n      x_map_coordinate\n      map_coordinates\n    \n  \n  \n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n10703689\nNA\nNA\nNA\nALPS\nRD\nPORTERS LAKE\n1\n2016\n1226\n1 Storey\n2\n2\nN\nAverage\nN\nN\nNA\nNA\nNA\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n8566615\n10\nNA\nNA\nSNEEZY\nAVE\nLAKE ECHO\n1\n1973\n768\nManufactured Home\n2\n1\nN\nManufactured Home\nN\nN\nNA\nNA\nNA\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n9781315\n108\nNA\nNA\nATIKIAN\nDR\nEASTERN PASSAGE\n1\n2008\n1400\n2 Storey\n3\n3\nN\nAverage\nN\nN\n44.61654\n-63.4618\n(44.61654, -63.4618)\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n8567875\n19\nNA\nNA\nTHE OTHER\nST\nPORTERS LAKE\n1\n1991\n896\nManufactured Home\n2\n1\nN\nManufactured Home\nN\nN\nNA\nNA\nNA\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n10535001\n15\nNA\nNA\nKINGS WHARF\nPL\nDARTMOUTH\n1\n2012\nNA\nNA\n2\n2\nN\nNA\nN\nN\nNA\nNA\nNA\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n10738822\n25\nNA\nNA\nALDERNEY\nDR\nDARTMOUTH\n1\n2016\nNA\nNA\n2\n2\nN\nNA\nN\nN\nNA\nNA\nNA"
  },
  {
    "objectID": "recipes/import-xlsx-range-cells.html",
    "href": "recipes/import-xlsx-range-cells.html",
    "title": "Read range of cells on excel import",
    "section": "",
    "text": "Description\n      \n    \n  \n  \n    Read a specific range of cells of an excel file\n\n  \n  \n  \n\n\n\n\n\n\n\n\n  \n    \n    \n  \n  \n    \n      Ingredients\n    \n    \n    \n      Package\n      Data\n    \n  \n  \n    readxl\n\nsample_housing_data.xlsx\n\n  \n  \n  \n\n\n\n\n\nSample Instructions\n\npackage::function(\"file_path/file_name.xlsx\", range = \"upper left cell ID:lower right cell ID\")\n\n\nActual Instructions\n\nreadxl::read_excel(\"C:/data/sample.xlsx\", range = \"B2:D5\")\n\n\nOutput\n\n\n\n\n\n\n  \n    \n      4610415\n      40\n      NA\n    \n  \n  \n    10535174\n15\nNA\n    822639\n40\nNA\n    9037063\n275\nNA"
  },
  {
    "objectID": "recipes/import-csv-macos.html",
    "href": "recipes/import-csv-macos.html",
    "title": "Import CSV File macOS",
    "section": "",
    "text": "Description\n      \n    \n  \n  \n    Basic method for reading a csv file on a macOS computer\n\n  \n  \n  \n\n\n\n\n\n\n\n\n  \n    \n    \n  \n  \n    \n      Ingredients\n    \n    \n    \n      Package\n      Data\n    \n  \n  \n    readr\n\nsample_dwelling_characteristics.csv\n\n  \n  \n  \n\n\n\n\n\nSample Instructions\n\npackage::function(\"file_path/file_name.csv\")\n\n\nActual Instructions\n\nreadr::read_csv(\"/data/sample_dwelling_characteristics.csv\")\n\n\nOutput\n\n\n\n\n\n\n  \n    \n      municipal_unit\n      assessment_account_number\n      civic_number\n      civic_additional\n      civic_direction\n      civic_street_name\n      civic_street_suffix\n      civic_city_name\n      living_units\n      year_built\n      square_foot_living_area\n      style\n      bedrooms\n      bathrooms\n      under_construction\n      construction_grade\n      finished_basement\n      garage\n      y_map_coordinate\n      x_map_coordinate\n      map_coordinates\n    \n  \n  \n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n10703689\nNA\nNA\nNA\nALPS\nRD\nPORTERS LAKE\n1\n2016\n1226\n1 Storey\n2\n2\nN\nAverage\nN\nN\nNA\nNA\nNA\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n08566615\n10\nNA\nNA\nSNEEZY\nAVE\nLAKE ECHO\n1\n1973\n768\nManufactured Home\n2\n1\nN\nManufactured Home\nN\nN\nNA\nNA\nNA\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n09781315\n108\nNA\nNA\nATIKIAN\nDR\nEASTERN PASSAGE\n1\n2008\n1400\n2 Storey\n3\n3\nN\nAverage\nN\nN\n44.61654\n-63.4618\n(44.61654, -63.4618)\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n08567875\n19\nNA\nNA\nTHE OTHER\nST\nPORTERS LAKE\n1\n1991\n896\nManufactured Home\n2\n1\nN\nManufactured Home\nN\nN\nNA\nNA\nNA\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n10535001\n15\nNA\nNA\nKINGS WHARF\nPL\nDARTMOUTH\n1\n2012\nNA\nNA\n2\n2\nN\nNA\nN\nN\nNA\nNA\nNA\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n10738822\n25\nNA\nNA\nALDERNEY\nDR\nDARTMOUTH\n1\n2016\nNA\nNA\n2\n2\nN\nNA\nN\nN\nNA\nNA\nNA"
  },
  {
    "objectID": "recipes/create-numeric-vector.html",
    "href": "recipes/create-numeric-vector.html",
    "title": "Create a Numeric Vector",
    "section": "",
    "text": "Description\n      \n    \n  \n  \n    Create a vector object in memory that has multiple numeric values\n\n  \n  \n  \n\n\n\n\n\n\n\n\n  \n    \n    \n  \n  \n    \n      Ingredients\n    \n    \n    \n      Package\n      Data\n    \n  \n  \n    NA\nNA\n  \n  \n  \n\n\n\n\n\nSample Instructions\n\nobject_name <- c(number, number, number)\n\n\nActual Instructions\n\nnumber_vector <- c(1, 2, 3)\n\n\nOutput\n\n\n[1] 1 2 3"
  },
  {
    "objectID": "recipes/create-character-variable.html",
    "href": "recipes/create-character-variable.html",
    "title": "Create a Character Variable",
    "section": "",
    "text": "Description\n      \n    \n  \n  \n    Create a variable object in memory for a specific character value\n\n  \n  \n  \n\n\n\n\n\n\n\n\n  \n    \n    \n  \n  \n    \n      Ingredients\n    \n    \n    \n      Package\n      Data\n    \n  \n  \n    NA\nNA\n  \n  \n  \n\n\n\n\n\nSample Instructions\n\nobject_name <- \"text\"\n\n\nActual Instructions\n\ncharacter_variable <- \"Halifax\""
  },
  {
    "objectID": "recipes/about-recipes.html",
    "href": "recipes/about-recipes.html",
    "title": "About Recipes",
    "section": "",
    "text": "The structure and design of this blog is based on iterative learning, starting with the most basic and build by adding one new element concept. Learning highly technical subjects can be challenging. A method to over come this challenge is to make a 1-to-1 comparison based on a four part idea:\n\nconcept (what)\npurpose (why)\nstructure (how)\nexample (how)\n\n\n\nLinear learning styles with the sole aim of becoming fluent in a language may result in the inability to perform basic and tangible actions desired by those wanting to communicate within that language  Johnny Harris\n\nThinking of adult learners and individuals learning on the job who are looking for help on a specific challenge, the blog has been structured to be small easily consumable chunks similar to that of a recipe card. The concept for a recipe card is that they are self contained, providing all the ingredients, preparation, and instructions required to create a meal. While a cookbook may consist of many recipes, there is no expectation to read, understand, and master all the recipes in order to prepare a meal. Following this as the central theme the blog, it has been designed as a number of data analytics recipes focusing on the R language.\n\nThe blog is built around a four course meal starting at the very beginning for individuals with no prior experience coding R, but looking to join the R data analytics community. In R there are many ways to undertake and tackle a problem, each with their own pros and cons. The examples, or recipes, in this blog are by no means the only, best, or most efficient means of performing a certain data analytics task. The approach taken is to build a base that can be easily built on using a similar style and coding syntax.\nThe sections, courses, are independent of each other, allowing a learner to begin according to their individual R data analytics journey, organized into four sections:\n\nAppetizers\nBreakfast\nLunch\nSupper\n\n\n\nSpicyness\n\nI love spicy food; tacos, burritos, chili, curry, hot sauce, almost anything, and I add hot sauce to almost everything. When beginning to eat spicy food you generally do not start with a Carolina Reaper, ghost pepper, or a habanero; most people may start with adding Frank’s Hot Sauce, a little chili powder, or even some fresh jalapenos to a meal. Similar to the progression of spicy food a spicy index was used to help communicate the relative difficulty of the analytical concept or technical coding difficulty.\n\n\n  \n\nNot spicy: Not technically difficult subject matter\n\n\n\n  \n\nMild spicy: Minimal technically difficult subject matter\n\n\n\n  \n\nModerate spicy: Moderate technically difficult subject matter\n\n\n\n  \n\nVery spicy: Technically difficult subject matter"
  },
  {
    "objectID": "recipes/import-csv-windows.html",
    "href": "recipes/import-csv-windows.html",
    "title": "Import CSV File Windows",
    "section": "",
    "text": "Description\n      \n    \n  \n  \n    Basic method for reading a csv file on a Windows computer\n\n  \n  \n  \n\n\n\n\n\n\n\n\n  \n    \n    \n  \n  \n    \n      Ingredients\n    \n    \n    \n      Package\n      Data\n    \n  \n  \n    readr\n\nsample_dwelling_characteristics.csv\n\n  \n  \n  \n\n\n\n\n\nSample Instructions\n\npackage::function(\"file_path/file_name.csv\")\n\n\nActual Instructions\n\nreadr::read_csv(\"C:/data/sample_dwelling_characteristics.csv\")\n\n\nOutput\n\n\n\n\n\n\n  \n    \n      municipal_unit\n      assessment_account_number\n      civic_number\n      civic_additional\n      civic_direction\n      civic_street_name\n      civic_street_suffix\n      civic_city_name\n      living_units\n      year_built\n      square_foot_living_area\n      style\n      bedrooms\n      bathrooms\n      under_construction\n      construction_grade\n      finished_basement\n      garage\n      y_map_coordinate\n      x_map_coordinate\n      map_coordinates\n    \n  \n  \n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n10703689\nNA\nNA\nNA\nALPS\nRD\nPORTERS LAKE\n1\n2016\n1226\n1 Storey\n2\n2\nN\nAverage\nN\nN\nNA\nNA\nNA\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n08566615\n10\nNA\nNA\nSNEEZY\nAVE\nLAKE ECHO\n1\n1973\n768\nManufactured Home\n2\n1\nN\nManufactured Home\nN\nN\nNA\nNA\nNA\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n09781315\n108\nNA\nNA\nATIKIAN\nDR\nEASTERN PASSAGE\n1\n2008\n1400\n2 Storey\n3\n3\nN\nAverage\nN\nN\n44.61654\n-63.4618\n(44.61654, -63.4618)\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n08567875\n19\nNA\nNA\nTHE OTHER\nST\nPORTERS LAKE\n1\n1991\n896\nManufactured Home\n2\n1\nN\nManufactured Home\nN\nN\nNA\nNA\nNA\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n10535001\n15\nNA\nNA\nKINGS WHARF\nPL\nDARTMOUTH\n1\n2012\nNA\nNA\n2\n2\nN\nNA\nN\nN\nNA\nNA\nNA\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n10738822\n25\nNA\nNA\nALDERNEY\nDR\nDARTMOUTH\n1\n2016\nNA\nNA\n2\n2\nN\nNA\nN\nN\nNA\nNA\nNA"
  },
  {
    "objectID": "recipes/import-csv-skip-blank-rows.html",
    "href": "recipes/import-csv-skip-blank-rows.html",
    "title": "Skip Blank Rows on CSV Import",
    "section": "",
    "text": "Description\n      \n    \n  \n  \n    Skip all rows that are blank when reading a csv file\n\n  \n  \n  \n\n\n\n\n\n\n\n\n  \n    \n    \n  \n  \n    \n      Ingredients\n    \n    \n    \n      Package\n      Data\n    \n  \n  \n    readr\n\nsample_dwelling_characteristics.csv\n\n  \n  \n  \n\n\n\n\n\nSample Instructions\n\npackage::function(\"file_path/file_name.csv\", skip_empty_rows = TRUE)\n\n\nActual Instructions\n\nreadr::read_csv(\"C:/data/sample_dwelling_characteristics.csv\", skip_empty_rows = TRUE)\n\n\nOutput\n\n\n\n\n\n\n  \n    \n      municipal_unit\n      assessment_account_number\n      civic_number\n      civic_additional\n      civic_direction\n      civic_street_name\n      civic_street_suffix\n      civic_city_name\n      living_units\n      year_built\n      square_foot_living_area\n      style\n      bedrooms\n      bathrooms\n      under_construction\n      construction_grade\n      finished_basement\n      garage\n      y_map_coordinate\n      x_map_coordinate\n      map_coordinates\n    \n  \n  \n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n10703689\nNA\nNA\nNA\nALPS\nRD\nPORTERS LAKE\n1\n2016\n1226\n1 Storey\n2\n2\nN\nAverage\nN\nN\nNA\nNA\nNA\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n08566615\n10\nNA\nNA\nSNEEZY\nAVE\nLAKE ECHO\n1\n1973\n768\nManufactured Home\n2\n1\nN\nManufactured Home\nN\nN\nNA\nNA\nNA\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n09781315\n108\nNA\nNA\nATIKIAN\nDR\nEASTERN PASSAGE\n1\n2008\n1400\n2 Storey\n3\n3\nN\nAverage\nN\nN\n44.61654\n-63.4618\n(44.61654, -63.4618)\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n08567875\n19\nNA\nNA\nTHE OTHER\nST\nPORTERS LAKE\n1\n1991\n896\nManufactured Home\n2\n1\nN\nManufactured Home\nN\nN\nNA\nNA\nNA\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n10535001\n15\nNA\nNA\nKINGS WHARF\nPL\nDARTMOUTH\n1\n2012\nNA\nNA\n2\n2\nN\nNA\nN\nN\nNA\nNA\nNA\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n10738822\n25\nNA\nNA\nALDERNEY\nDR\nDARTMOUTH\n1\n2016\nNA\nNA\n2\n2\nN\nNA\nN\nN\nNA\nNA\nNA"
  },
  {
    "objectID": "recipes/create-variable-function.html",
    "href": "recipes/create-variable-function.html",
    "title": "Create Variable from a Function",
    "section": "",
    "text": "Description\n      \n    \n  \n  \n    Create a variable object in memory from the output from the system date function\n\n  \n  \n  \n\n\n\n\n\n\n\n\n  \n    \n    \n  \n  \n    \n      Ingredients\n    \n    \n    \n      Package\n      Data\n    \n  \n  \n    NA\nNA\n  \n  \n  \n\n\n\n\n\nSample Instructions\n\nobject_name <- function()\n\n\nActual Instructions\n\ncurrent_date_variable <- Sys.Date()"
  },
  {
    "objectID": "recipes/import-csv-skip-rows.html",
    "href": "recipes/import-csv-skip-rows.html",
    "title": "Skip Rows on CSV Import",
    "section": "",
    "text": "Description\n      \n    \n  \n  \n    Skip a number of rows when reading a csv file\n\n  \n  \n  \n\n\n\n\n\n\n\n\n  \n    \n    \n  \n  \n    \n      Ingredients\n    \n    \n    \n      Package\n      Data\n    \n  \n  \n    readr\n\nsample_dwelling_characteristics.csv\n\n  \n  \n  \n\n\n\n\n\nSample Instructions\n\npackage::function(\"file_path/file_name.csv\", skip = number)\n\n\nActual Instructions\n\nreadr::read_csv(\"C:/data/sample_dwelling_characteristics.csv\", skip = 2)\n\n\nOutput\n\n\n\n\n\n\n  \n    \n      HALIFAX REGIONAL MUNICIPALITY (HRM)\n      08566615\n      10\n      NA...4\n      NA...5\n      SNEEZY\n      AVE\n      LAKE ECHO\n      1...9\n      1973\n      768\n      Manufactured Home...12\n      2\n      1...14\n      N...15\n      Manufactured Home...16\n      N...17\n      N...18\n      NA...19\n      NA...20\n      NA...21\n    \n  \n  \n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n09781315\n108\nNA\nNA\nATIKIAN\nDR\nEASTERN PASSAGE\n1\n2008\n1400\n2 Storey\n3\n3\nN\nAverage\nN\nN\n44.61654\n-63.46180\n(44.61654, -63.4618)\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n08567875\n19\nNA\nNA\nTHE OTHER\nST\nPORTERS LAKE\n1\n1991\n896\nManufactured Home\n2\n1\nN\nManufactured Home\nN\nN\nNA\nNA\nNA\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n10535001\n15\nNA\nNA\nKINGS WHARF\nPL\nDARTMOUTH\n1\n2012\nNA\nNA\n2\n2\nN\nNA\nN\nN\nNA\nNA\nNA\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n10738822\n25\nNA\nNA\nALDERNEY\nDR\nDARTMOUTH\n1\n2016\nNA\nNA\n2\n2\nN\nNA\nN\nN\nNA\nNA\nNA\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n00848328\n147\nNA\nNA\nBROOKS\nDR\nEAST PRESTON\n1\n1982\n1761\nSplit Entry\nNA\n1\nN\nFair\nY\nY\n44.72310\n-63.43018\n(44.7231, -63.43018)\n    HALIFAX REGIONAL MUNICIPALITY (HRM)\n05599393\n87\nNA\nNA\nBIRCHILL\nDR\nEASTERN PASSAGE\n1\n1983\n896\nManufactured Home\n2\n1\nN\nManufactured Home\nN\nN\nNA\nNA\nNA"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Data and analytics are becoming important aspects in more organizations, with many making significant investments in the data domain. Utilizing data and analytics can help organizations or any size and from all industries. However, the implementation and transformation toward a data driven organization has tended to focus on needs of large tech-based organizations. Much of the data transformation journey has focused on highly technical areas like big data, real-time data streaming, machine learning, and artificial intelligence. While extremely important, powerful, and important, not all organizations require the bleeding edge of data analytics. This over emphasis has made it very difficult for smaller organizations, or those early in their data maturity to implement data analytics into their organizational culture.\n\nData Chef is looking to help by demystifying the data and analytics domain through knowledge sharing with data content designed for a wide range of users, not just those with high technical abilities and understanding. A wide range of content covering data literacy, data governance, data management, and analytics, for all…not just large tech organizations.\n\nIf you have data needs reach out via email"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Chef",
    "section": "",
    "text": "Serving data knowledge for everyone through content and strategy"
  },
  {
    "objectID": "stories/delivering-products-faster.html",
    "href": "stories/delivering-products-faster.html",
    "title": "Delivering Data Products Faster",
    "section": "",
    "text": "Posit Connect has empowered the Halifax International Airport Authority (HIAA) to deliver products following a rapid prototyping methodology, putting versions of a product in the hands of users in days – allowing for actionable feedback and updates that are back to users in hours.\nIn a few short months after implementing Posit Connect, their team has achieved numerous successes. The range of data-driven content available to users expanded from just two dashboards to include:\n\nInteractive applications (e.g. flight schedule search)\nDynamic dashboards (e.g. passenger volume, YHZ Express utilization)\nA knowledge centre focused on data analytics learning materials\n\nPosit Connect, being built with open-source architectures in mind means that we can easily integrate it with our data platform, allowing users to interact with HIAA Data Analytics Hub as a store front for accessing analytical content and data artifacts, opening the door to a wider range of users. This code-first approach, combined with the extensive available R libraries, also allows for integrations with Microsoft Teams, our organization’s internal communication platform.\nFull Story on Posit"
  },
  {
    "objectID": "stories/choosing-ev.html",
    "href": "stories/choosing-ev.html",
    "title": "Choosing an Electric Vehicle",
    "section": "",
    "text": "Electric vehicles (EV) are increasing in popularity, by both the consumer and automakers. There are a significant number of EV available on the market, each having different features and characteristics. With EVs being relatively new to the vehicle market, it can be very difficult to know which EVs to consider as a new vehicle. Within this story we will present a data driven approach for making an informed decision when looking for new electric vehicles. A number of EVs will be evaluated based on objective features, omitting things like appearance, style, etc. A custom analytical model will be created to analyze the features and suggest the top electric vehicles."
  },
  {
    "objectID": "stories/choosing-ev.html#feature-engineering",
    "href": "stories/choosing-ev.html#feature-engineering",
    "title": "Choosing an Electric Vehicle",
    "section": "Feature Engineering",
    "text": "Feature Engineering\nThe format of the data collected on the EVs is not conducive to support the design of the model. Rather than modifying the source data (i.e. changing the values to reflected the desired data format values) new features, or attributes, will be created using the source values.\n\nFeature engineering entails reformatting predictor values to make them easier for a model to use effectively. This includes transformations and encoding of the data to be represent their important characteristics  Tidy Modeling with R  Max Kuhn & Julie Silge\n\nSource data elements were used to created three features for use in the model. New features were encoding using values of 0, 1, 3, 6, and 9 providing a clear distinction between the values of the feature.\n\nPriceRangeDrive TrainEngineered Dataset\n\n\nThe following was used when creating new price features\n\n\n\nPrice Range\nFeature Encoding\n\n\n\n\nLess than $30,000\n9\n\n\n$30,000 to $40,000\n6\n\n\n$40,000 to $50,000\n3\n\n\n$50,000 to $60,000\n1\n\n\nGreater than $60,000\n0\n\n\n\n\n# Function to create new feature\n# Specific to electric vehicle price\ncreate_price_feature <- function(.data, price_column){\n  .data |>\n    dplyr::mutate(price_value = dplyr::case_when(\n    {{price_column}} < 30000 ~ 9,\n    dplyr::between({{price_column}}, 30000, 40000) ~ 6,\n    dplyr::between({{price_column}}, 40000, 50000) ~ 3,\n    dplyr::between({{price_column}}, 50000, 60000) ~ 1,\n    {{price_column}} > 60000 ~ 0\n  ))\n}\n\n\n\nThe following was used when creating new driving range (in kilometres) features\n\n\n\nDriving Range\nFeature Encoding\n\n\n\n\nGreater than 500km\n9\n\n\n400 to 500km\n6\n\n\n300 to 400km\n3\n\n\n200 to 300km\n1\n\n\nLess than 200km\n0\n\n\n\n\n# Function to create new feature\n# Specific to electric vehicle range\ncreate_range_feature <- function(.data, range_column){\n  .data |>\n    dplyr::mutate(range_value = dplyr::case_when(\n    {{range_column}} > 500 ~ 9,\n    dplyr::between({{range_column}}, 400, 500) ~ 6,\n    dplyr::between({{range_column}}, 300, 400) ~ 3,\n    dplyr::between({{range_column}}, 200, 300) ~ 1,\n    {{range_column}} < 200 ~ 0\n  ))\n}\n\n\n\nThe following was used when creating new drive train features\n\n\n\nDrive Train\nFeature Encoding\n\n\n\n\nFour wheel drive (4WD)\n9\n\n\nAll wheel drive (AWD)\n6\n\n\nFront wheel drive (FWD)\n3\n\n\nRear wheel drive (RWD)\n1\n\n\n\n\n# Function to create new feature\n# Specific to electric vehicle drivetrain\ncreate_drivetrain_feature <- function(.data, drivetrain_column){\n  .data |>\n    dplyr::mutate(drivetrain_value = dplyr::case_when(\n    {{drivetrain_column}} == \"4WD\" ~ 9,\n    {{drivetrain_column}} == \"AWD\" ~ 6,\n    {{drivetrain_column}} == \"FWD\" ~ 3,\n    {{drivetrain_column}} == \"RWD\" ~ 1\n  ))\n}\n\n\n\n\n\n\n\nvehicles |>\n  create_drivetrain_feature(drivetrain) |>\n  create_range_feature(range) |>\n  create_price_feature(price) |>\n  DT::datatable()"
  },
  {
    "objectID": "stories/choosing-ev.html#designing-a-model",
    "href": "stories/choosing-ev.html#designing-a-model",
    "title": "Choosing an Electric Vehicle",
    "section": "Designing a Model",
    "text": "Designing a Model\nThe idea behind the model is to combine the three variables into a single value that can be used to objectively suggest which EVs meet the needs of the user. Using this idea, there are two methods to combine the variables, adding or multiplying. For this story two models will be designed, one using an additive approach, and the other using a multiply approach.\nA critical aspect of the model is to have a weighting factor for each of the variables. The thought behind this was that some variables are more important than others.\n\nweights <- list(price = 0.6, range = 0.3, drivetrain = 0.1)\n\nModel Variable Weights\n\n\n\nVariable\nWeight\n\n\n\n\nPrice\n0.6\n\n\nRange\n0.3\n\n\nDrivetrain\n0.1\n\n\n\n\n\nModel One\nThe approach of this model is to apply the variable weight to each variable and add each variable together\n\n\\[\n(price * weightPrice) + (range * weightRange) + (drivetrain * weightDrivetrain)\n\\]\n\n\nBuilding Model OneModel One OutputVisualizing Model One\n\n\n\nmodel1 <- function(.data, manufacture, model, price, range, drivetrain){\n  .data |>\n    dplyr::mutate(model_output = ({{price}} * weights$price) + ({{range}} * weights$range) + ({{drivetrain}} * weights$drivetrain)) |>\n    dplyr::select({{manufacture}}, {{model}}, model_output)\n}\n\n\n\n\n# Applying model one to the vehicle dataset\nvehicles |>\n  model1(manufacture = make,\n         model = model,\n         price = price_value,\n         range = range_value,\n         drivetrain = drivetrain_value)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe top five electric vehicles identified from Model One are Chevy Bolt EV, Hyundai Kona Ultimate, Kia Niro EV, Kia Niro PHEV, Toyota Prius Prime\n \n\n\nModel Two\nThe approach of this model is to apply the variable weight to each variable and multiply each variable together\n\n\\[\n(price * weightPrice) * (range * weightRange) * (drivetrain * weightDrivetrain)\n\\]\n\n\nBuilding Model TwoModel Two OutputVisualizing Model Two\n\n\n\nmodel2 <- function(.data, manufacture, model, price, range, drivetrain){\n  .data |>\n    dplyr::mutate(model_output = ({{price}} * weights$price) * ({{range}} * weights$range) * ({{drivetrain}} * weights$drivetrain)) |>\n    dplyr::select({{manufacture}}, {{model}}, model_output)\n}\n\n\n\n\n# Applying model two to the vehicle dataset\nvehicles |>\n  model2(manufacture = make,\n         model = model,\n         price = price_value,\n         range = range_value,\n         drivetrain = drivetrain_value)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe top five electric vehicles identified from Model Two are Chevy Bolt EV, Hyundai Kona Ultimate, Kia Niro EV, Hyundai Ioniq 5 Preferred AWD Long Range, Nissan Ariya, VW ID.4 Pro AWD"
  },
  {
    "objectID": "stories/speed-up-analytcs.html",
    "href": "stories/speed-up-analytcs.html",
    "title": "Speed Up Data Analytics",
    "section": "",
    "text": "Data is increasing in value for many organizations — an asset leveraged to help make informed business decisions. Unfortunately, this sentiment has not been the norm throughout the life of most organizations, with vast amounts of data locked in legacy data management systems and designs. The majority of organizations use relational database management systems (RDBMS) like Oracle, Postgres, Microsoft SQL, or MySQL to store and manage their enterprise data. Typically these systems were designed to collect and process data quickly within a transactional data model. While these models are excellent for applications, they can pose challenges for performing business intelligence, data analytics, or predictive analysis. Many organizations are realizing their legacy systems are not sufficient for data analytics initiatives, providing an opportunity for analytics teams to present tangible options to improve their organization’s data analytics infrastructure.\nRegardless if you are engineering data for others to consume for analysis, or performing the analytics, reducing the time to perform data processing is critically important. Within this post, we are going to evaluate the performance of two distinct data storage formats; row-based (CSV) and columnar (parquet); with CSV being a tried and tested standard data format used within the data analytics field, and parquet becoming a viable alternative in many data platforms.\nFull story on Posit"
  },
  {
    "objectID": "stories/geospatial-distributed-processing.html",
    "href": "stories/geospatial-distributed-processing.html",
    "title": "Geospatial Distributed Processing",
    "section": "",
    "text": "Everything happens somewhere. Location is an important factor when people make decisions that influences various life decisions, from buying a house, choosing a school, to vacations, or selecting a restaurant to eat out at. Within the data space utilizing location data is becoming increasingly important in data analysis, data science, and machine learning. Data wrangling is an integral part of data science, typically occupying a significant portion of time and effort, with location data adding to the complexity.\nLocation data can be large, consisting of hundreds of thousands to billions of observations in a single dataset. With location-based analysis consistently utilizing multiple datasets, the time to complete the analysis can be lengthy. Similar to other data analytics, the majority of location-based analysis tasks are performed sequentially using a single processor on a computer, even though many of the operations performed have no dependency on the other data or their outcome; if only there were another way…well, let me introduce you to distributed location-based processing.\nWhat is distributed processing? In short, distributed processing is the use of multiple computer processors to execute a single computational task. A distributed process is executed by splitting a task into smaller chunks which are simultaneously performed on multiple processors. We can use distributed processing to make our code run faster, sometimes much faster. For large geospatial datasets that require a lot of computational power, distributed processing can reduce the time needed to handle complex data manipulation.\nIn this post, we’ll explore different techniques for wrangling location data, the processing time required for each, and the significant improvements that can be seen with distributed processing.\nFull story on Posit"
  },
  {
    "objectID": "farm-to-table.html",
    "href": "farm-to-table.html",
    "title": "Data Chef",
    "section": "",
    "text": "Data is becoming increasingly important to more organizations, however understanding the lifecycle and all the different aspects that are associated to the data space can be challenging. Data literacy is an important, under appreciated, component of a data organization. The concept of farm to table is used to explain the stages and progression within a data related initiative.\n\nFarm to table  Noting or relating to the stages involved in the growing, processing, and consumption of food\n\n\n\n\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nGrow\n\n\n\n\n\n\n\nfarming\n\n\ndata collection\n\n\n \n\n\n\n\n2023-02-04\n\n\nRyan Garnett\n\n\n1 min\n\n\n\n\n\n\n\n\nHarvest\n\n\n\n\n\n\n\nfarming\n\n\ndata engineering\n\n\n \n\n\n\n\n2023-02-05\n\n\nRyan Garnett\n\n\n0 min\n\n\n\n\n\n\n\n\nShip\n\n\n\n\n\n\n\nsupply chain\n\n\ndata engineering\n\n\n \n\n\n\n\n2023-02-06\n\n\nRyan Garnett\n\n\n0 min\n\n\n\n\n\n\n\n\nProcess\n\n\n\n\n\n\n\nsupply chain\n\n\ndata engineering\n\n\n \n\n\n\n\n2023-02-07\n\n\nRyan Garnett\n\n\n0 min\n\n\n\n\n\n\n\n\nPackage\n\n\n\n\n\n\n\nsupply chain\n\n\ndata engineering\n\n\n \n\n\n\n\n2023-02-08\n\n\nRyan Garnett\n\n\n0 min\n\n\n\n\n\n\n\n\nStore\n\n\n\n\n\n\n\nsupply chain\n\n\ndata engineering\n\n\n \n\n\n\n\n2023-02-09\n\n\nRyan Garnett\n\n\n0 min\n\n\n\n\n\n\n\n\nAccess\n\n\n\n\n\n\n\ncooking\n\n\ndata engineering\n\n\n \n\n\n\n\n2023-02-10\n\n\nRyan Garnett\n\n\n0 min\n\n\n\n\n\n\n\n\nPrepare\n\n\n\n\n\n\n\ncooking\n\n\ndata wrangling\n\n\n \n\n\n\n\n2023-02-11\n\n\nRyan Garnett\n\n\n0 min\n\n\n\n\n\n\n\n\nCook\n\n\n\n\n\n\n\ncooking\n\n\ndata analytics\n\n\n \n\n\n\n\n2023-02-12\n\n\nRyan Garnett\n\n\n0 min\n\n\n\n\n\n\n\n\nPlate\n\n\n\n\n\n\n\ncooking\n\n\ndata analytics\n\n\n \n\n\n\n\n2023-02-13\n\n\nRyan Garnett\n\n\n0 min\n\n\n\n\n\n\n\n\nDeliver\n\n\n\n\n\n\n\ncustomer experience\n\n\ninformation delivery\n\n\n \n\n\n\n\n2023-02-14\n\n\nRyan Garnett\n\n\n0 min\n\n\n\n\n\n\n\n\nEat\n\n\n\n\n\n\n\ncustomer experience\n\n\ninformed decision\n\n\n \n\n\n\n\n2023-02-15\n\n\nRyan Garnett\n\n\n0 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/cook.html",
    "href": "posts/cook.html",
    "title": "Cook",
    "section": "",
    "text": "The most glamorous part of farm to table journey is cooking; it is where the magic happens, the transformation of a raw source into something tasty. There are many different approaches, methods, and considerations that are needed when cooking a meal. Generally a cooking method is chosen (BBQ, bake, boil, fry, etc.) which is incorporated into series of steps outlined in the recipe .\n\nThe data equivalent is data analysis within the area of data analytics. Following the journey from collection, engineering, and wrangling, the data is ready for analysis. There are many different analysis methods available, each with benefits and limitations:\n\nclassification\ndescriptive\ngeospatial\ninferential\nmachine learning / artificial intelligence\npredictive\nprescriptive\n\nSimilar with cooking a meal where the cooking method needs to be paired with the appropriate food and situation, the analysis method used should fit the need of the question to be explored and the available data.\n\nKey points\n\nidentify the question that is to be explored\nselect the method that suits the needs\nensure their is adequate volume of data"
  },
  {
    "objectID": "posts/prepare.html",
    "href": "posts/prepare.html",
    "title": "Prepare",
    "section": "",
    "text": "Prior to cooking most food require some level of preparation, such as being pealed, copped, sliced, mixed, or marinaded. The preparation may include phases where multiple ingredients are combined together before the cooking portion begins. The steps taken will depend on the recipe that is being cooked, as not all meals require the same type, or level, of preparation.\n\nThe data equivalent is data wranging. The extraction of data is the starting point of data pipelines. Similar to harvesting plants there are considerations when extracting data, such as what data is to be extracted (all tables from a source, specific columns), when the extraction should occur (specific time, reoccurring time, ad hoc), how the data should be extracted (manual vs. automated), and where is the data being extracted from (structured/unstructured data, database, file, service, etc.). Understanding these components will assist with developing a data extraction method.\n\nKey points\n\nselect the extraction method that fits the data source\ndesign for the data source\ndetermine the intended usage pattern"
  },
  {
    "objectID": "posts/plate.html",
    "href": "posts/plate.html",
    "title": "Plate",
    "section": "",
    "text": "When the plants reach the end of their growing season the harvest begins. A harvest can take many forms, depending on the food that is being harvested. A single harvest method will not apply to all food sources, some can use large machinery, while others require delicate manual interaction. Harvest time is also a significant consideration, as certain plants need to be harvested immediately, whereas others have a longer grace period.\n\nThe data equivalent is data presentation within the area of data analytics. There is a tight connection between the analysis, presentation, and delivery of the data. The delivery must be representative of the analysis, while reflecting the available methods of delivery. There are many important elements to consider in order to present data effectively, which are tightly related to the analysis performed and the available delivery method:\n\ndynamic vs. static\ndata vs. dashboard vs. report\nKPI or metric\n\n\nKey points\n\nunderstand the audience and purpose\nensure selected method matches available delivery options\ndetermine the expected level of detail required"
  },
  {
    "objectID": "posts/eat.html",
    "href": "posts/eat.html",
    "title": "Eat",
    "section": "",
    "text": "It is time to enjoy the meal. Eating food comes in many forms, from a snack, quick lunch, family supper, to a large multi course special occasion, with each having a specific purpose and people attending. The reason for eating has an influence on number of people, the frequency, and re-occurrence of the meal.\n\nThe data equivalent is informed decision within the area of customer experience. The use of data to help inform an objective decision is important for many organizations. There are a number of factors and considerations when making an informed decision:\n\nfrequency of the decision\n\nad hoc decision\nre-occurring decision\n\naudience\n\nindividual decision maker\ngroup decision\n\nsize and scope of the decision\nresult and outcome\n\ndecision is final\ndecision leads to additional analysis\n\n\n\nKey points\n\nunderstand the magnitude of the decision\nidentify the audience\nidentify frequency and re-occurrence of decision"
  },
  {
    "objectID": "posts/grow.html",
    "href": "posts/grow.html",
    "title": "Grow",
    "section": "",
    "text": "The food cycle starts with planting seeds. There is more to growing food the just putting seeds in the ground and letting them grow. There are many factors that are required to be successful many factors. Select the location that as the proper amount of sun, the correct soil type, and the required space is one of the first consideration needed. Once the planting location has been selected the growing area needs to be prepared, by adding extra nutrients to help plants grow within their early stages. While plants are growing they require attention such as watering, weeding, and additional nutrients based on their growth.\n\nThe data equivalent is data collection. The collection of data is the beginning of all data journeys, and like growing plants, there are many things to consider and undertake. Data collection follows five basic questions: what, who, when, where, and how. \nWhat data needs to be collected. For example, user feedback, traffic volumes, environmental conditions, product usage, etc.\nWho is the target “audience” of the data collection\nWhen should the data be collected\nWhere is the optimal location to be collecting the data\nHow will the data be collected, the method or instrument that will be used to collect the data (i.e. survey, interview, sensor, field collection, etc.)\n\nKey points\n\nidentify the audience\nselect the collection method\nstructure the inputs that are being collected"
  },
  {
    "objectID": "posts/package.html",
    "href": "posts/package.html",
    "title": "Package",
    "section": "",
    "text": "After the plants have undergone processing they are put into an appropriate package to assist with the next stages of the journey. The packaging is dependent on the type of processing that was performed. Reconsidering an the apple from processing, different containers will be chosen for apple juice than that of apple sauce, frozen apple slices, or apples that will be sold as fresh produce.\n\nThe data equivalent is data model within the area of data engineering. Like with food data is better consumed when the packaging matches it’s intended use. When creating a data model considerations of how the data will be used (i.e. record management, analytics, relationships, etc.) will influence what data model (transactional, analytical, graph, relational) is required. Many data usage cases cannot be resolved with a single data model, meaning multiple data models may be needed.\n\nKey points\n\ndetermine all potential uses of the data\nidentify the appropriate data model(s)\ndesign the data model structure and elements"
  },
  {
    "objectID": "posts/process.html",
    "href": "posts/process.html",
    "title": "Process",
    "section": "",
    "text": "After being shipped from the farm the plants are evaluated to identify their relative quality, which determines the potential usage. Consider an apple, various factors (i.e. shape, visual appearance, size, number of bruises, etc.) will decide its end usage. That decision will set in motion if the apple should be juiced, pureed into apple sauce, cut and frozen, or used as-is.\n\nThe data equivalent is data transformation within the area of data engineering. In many cases the data collected is not ready for consumption in all use cases. Like with food, data needs to be profiled to identify any defects and determine what transformations (reshape, filter, aggregate, summarize, etc.), what data quality issues (data types, missing values, values out-of-range, etc.), and how to clean identified dirty data.\n\nKey points\n\nperform data profiling\nreview quality of data\nreshape, transform, and clean data"
  },
  {
    "objectID": "posts/ship.html",
    "href": "posts/ship.html",
    "title": "Ship",
    "section": "",
    "text": "Following the harvest the plants enter a new journey and are moved from the farm. The plants are bundled and placed into a vehicle to be transported to the next stage. The method and length of the transport depends on the crop, as some require refrigeration, while others do not.\n\nThe data equivalent is data transit within the area of data engineering. The purpose of data transit is to move the data from one stage to another. Like with shipping food considerations on how the data is moved (streaming vs. file transfer), and when the data needs to be moved (constantly, or in pieces) is needed in order to build a data pipeline that meets the requirements of the next stage and the users.\n\nKey points\n\nchoose the method that matches the data\nidentify the volume of data to be moved\nunderstand the speed needed to move the data"
  },
  {
    "objectID": "posts/access.html",
    "href": "posts/access.html",
    "title": "Access",
    "section": "",
    "text": "The beginning of the cooking journey starts with accessing food. Where the food is accessed from, and the amount, has a connection to both what is being prepared, and who is making the meal. There is a significant difference between a four course meal being prepared at a five star restaurant, and someone looking to make a sandwich for their lunch. These differences are reflective in where the food is accessed from.\n\nThe data equivalent is data access within the area of data engineering. Access to data is an important factor that is typically overlooked. Considerations on who is accessing the data (application or user), what technical abilities they have, and how much data is required is essential for determining the different methods of access, which may include:\n\napplication programming interface (API)\ndirect connection (database)\ndownload (file)\nmanaged service\n\n\nKey points\n\nidentify the type of users\nunderstand user’s technical abilities\ndetermine volume of data needed"
  },
  {
    "objectID": "posts/store.html",
    "href": "posts/store.html",
    "title": "Store",
    "section": "",
    "text": "The final stage in a food’s supply chain journey is storage. A number of options are available (i.e. market, grocery store, warehouse, etc.) each designed and suited for different food choices. Following the example of the apple from previous posts, a bag of frozen apple slices would be better suited for a grocery store or warehouse, rather than a farmer’s market.\n\nThe data equivalent is data storage within the area of data engineering. Selecting a data storage system that is not inline with the intended use case can introduce significant limitations and challenges. When deciding on a data storage system there are a number of factors to consider, such as:\n\ncloud or on-premise or hybrid\nenterprise or local\nstructured or unstructured\ndatabase or file\nrow or columnar\n\n\nKey points\n\nunderstand the type of use\nunderstand the usage volume\nunderstand the volume of data"
  },
  {
    "objectID": "posts/deliver.html",
    "href": "posts/deliver.html",
    "title": "Deliver",
    "section": "",
    "text": "When the meal is complete the last stage in the food journey is delivery. There are different options to getting a meal in the hands of a customer, such as dine-in, take-out, or self-serve (i.e. buffet). The different delivery methods contribute to the overall customer experience, requiring different levels of involvement from customers.\n\nThe data equivalent is information delivery within the area of customer experience. Getting the analysis in the hands of users can take many forms, depending on the findings, and the audience. Choosing a delivery method should be reflective of both the analysis and the user’s needs and abilities. Common data delivery methods are:\n\napplication\ndata service\nemail attachment\nnotification\nscheduled\nself-service\n\n\nKey points\n\nunderstand the audience\nidentify the frequency\nunderstand limitations and challenges"
  },
  {
    "objectID": "posts/harvest.html",
    "href": "posts/harvest.html",
    "title": "Harvest",
    "section": "",
    "text": "When the plants reach the end of their growing season the harvest begins. A harvest can take many forms, depending on the food that is being harvested. A single harvest method will not apply to all food sources, some can use large machinery, while others require delicate manual interaction. Harvest time is also a significant consideration, as certain plants need to be harvested immediately, whereas others have a longer grace period.\n\nThe data equivalent is data extraction within the area of data engineering. The extraction of data is the starting point of data pipelines. Similar to harvesting plants there are considerations when extracting data, such as what data is to be extracted (all tables from a source, specific columns), when the extraction should occur (specific time, reoccurring time, ad hoc), how the data should be extracted (manual vs. automated), and where is the data being extracted from (structured/unstructured data, database, file, service, etc.). Understanding these components will assist with developing a data extraction method.\n\nKey points\n\nselect the extraction method that fits the data source\ndesign for the data source\ndetermine the intended usage pattern"
  },
  {
    "objectID": "data-stories.html",
    "href": "data-stories.html",
    "title": "Data Chef",
    "section": "",
    "text": "Communicating the findings from a data analysis is fairly standard, however explaining the full data journey to an audience is a relatively new area within the data field; enter Data Stories. The goal of a data story is to share the process, methods, analysis, and findings in a reproducible approach. Data Chef data stories are geared for a wide range of users, but lean on the more technical side, consisting of code snippets (note the code is not intended to be production quality, meaning considerations around optimization, testing, validation, etc. are not the main focus). The focus of these stories are to illustrate approaches for using data and analytics techniques to support informed decisions.\n\n\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nSpeed Up Data Analytics\n\n\n\n\n\n\n\ndata\n\n\ndata analytics\n\n\ndata wrangling\n\n\nParquet\n\n\n \n\n\n\n\n2022-04-26\n\n\nRyan Garnett, Ray Wong, Dan Reed\n\n\n1 min\n\n\n\n\n\n\n\n\nGeospatial Distributed Processing\n\n\n\n\n\n\n\ndata\n\n\ndistributed processing\n\n\ngeospatial\n\n\n \n\n\n\n\n2023-01-23\n\n\nRyan Garnett\n\n\n1 min\n\n\n\n\n\n\n\n\nChoosing an Electric Vehicle\n\n\n\n\n\n\n\nEV\n\n\nmodelling\n\n\n \n\n\n\n\n2023-04-14\n\n\nRyan Garnett\n\n\n16 min\n\n\n\n\n\n\n\n\nDelivering Data Products Faster\n\n\n\n\n\n\n\ndata\n\n\nHIAA\n\n\nPosit\n\n\nproduct\n\n\n \n\n\n\n\n2024-01-01\n\n\nRyan Garnett\n\n\n0 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "data-recipes.html",
    "href": "data-recipes.html",
    "title": "Data Chef",
    "section": "",
    "text": "The concept for a recipe card is that they are self contained, providing all the ingredients, preparation, and instructions required to create a meal. This approach will be used to explain how to do various data related activities within RStudio.\n\n\n\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nAbout Recipes\n\n\n\n\n\n\n\nabout\n\n\n \n\n\n\n\n2023-03-29\n\n\nRyan Garnett\n\n\n2 min\n\n\n\n\n\n\n\n\nCreate a Data Frame\n\n\n\n\n\n\n\nappetizer\n\n\ncreate data frame\n\n\nnone spicy\n\n\n \n\n\n\n\n2023-03-29\n\n\nRyan Garnett\n\n\n4 min\n\n\n\n\n\n\n\n\nCreate a Numeric Variable\n\n\n\n\n\n\n\nappetizer\n\n\ncreate variable\n\n\nnone spicy\n\n\n \n\n\n\n\n2023-03-29\n\n\nRyan Garnett\n\n\n2 min\n\n\n\n\n\n\n\n\nCreate Variable from an Equation\n\n\n\n\n\n\n\nappetizer\n\n\ncreate variable\n\n\nnone spicy\n\n\n \n\n\n\n\n2023-03-29\n\n\nRyan Garnett\n\n\n2 min\n\n\n\n\n\n\n\n\nCreate Variable from Combining Variables\n\n\n\n\n\n\n\nappetizer\n\n\ncreate variable\n\n\nnone spicy\n\n\n \n\n\n\n\n2023-03-29\n\n\nRyan Garnett\n\n\n2 min\n\n\n\n\n\n\n\n\nCreate a Character Variable\n\n\n\n\n\n\n\nappetizer\n\n\ncreate variable\n\n\nnone spicy\n\n\n \n\n\n\n\n2023-03-29\n\n\nRyan Garnett\n\n\n2 min\n\n\n\n\n\n\n\n\nCreate Variable from a Function\n\n\n\n\n\n\n\nappetizer\n\n\ncreate variable\n\n\nnone spicy\n\n\n \n\n\n\n\n2023-03-29\n\n\nRyan Garnett\n\n\n2 min\n\n\n\n\n\n\n\n\nCreate a Character Vector\n\n\n\n\n\n\n\nappetizer\n\n\ncreate vector\n\n\nnone spicy\n\n\n \n\n\n\n\n2023-03-29\n\n\nRyan Garnett\n\n\n2 min\n\n\n\n\n\n\n\n\nCreate a Numeric Vector\n\n\n\n\n\n\n\nappetizer\n\n\ncreate vector\n\n\nnone spicy\n\n\n \n\n\n\n\n2023-03-29\n\n\nRyan Garnett\n\n\n2 min\n\n\n\n\n\n\n\n\nImport CSV File Linux\n\n\n\n\n\n\n\nappetizer\n\n\nimport data\n\n\nmild spicy\n\n\n \n\n\n\n\n2023-03-31\n\n\nRyan Garnett\n\n\n2 min\n\n\n\n\n\n\n\n\nImport Excel File\n\n\n\n\n\n\n\nappetizer\n\n\nimport data\n\n\nmild spicy\n\n\n \n\n\n\n\n2023-04-03\n\n\nRyan Garnett\n\n\n2 min\n\n\n\n\n\n\n\n\nImport CSV File from a Repository\n\n\n\n\n\n\n\nappetizer\n\n\nimport data\n\n\nmild spicy\n\n\n \n\n\n\n\n2023-03-31\n\n\nRyan Garnett\n\n\n2 min\n\n\n\n\n\n\n\n\nRead number of rows on excel import\n\n\n\n\n\n\n\nappetizer\n\n\nimport data\n\n\nmild spicy\n\n\n \n\n\n\n\n2023-04-03\n\n\nRyan Garnett\n\n\n2 min\n\n\n\n\n\n\n\n\nRead Number of Rows on CSV Import\n\n\n\n\n\n\n\nappetizer\n\n\nimport data\n\n\nmild spicy\n\n\n \n\n\n\n\n2023-03-31\n\n\nRyan Garnett\n\n\n2 min\n\n\n\n\n\n\n\n\nSkip Rows on Excel Import\n\n\n\n\n\n\n\nappetizer\n\n\nimport data\n\n\nmild spicy\n\n\n \n\n\n\n\n2023-04-03\n\n\nRyan Garnett\n\n\n2 min\n\n\n\n\n\n\n\n\nRead Specific Sheet on Excel Import\n\n\n\n\n\n\n\nappetizer\n\n\nimport data\n\n\nmild spicy\n\n\n \n\n\n\n\n2023-04-03\n\n\nRyan Garnett\n\n\n2 min\n\n\n\n\n\n\n\n\nRead range of cells on excel import\n\n\n\n\n\n\n\nappetizer\n\n\nimport data\n\n\nmild spicy\n\n\n \n\n\n\n\n2023-04-03\n\n\nRyan Garnett\n\n\n2 min\n\n\n\n\n\n\n\n\nImport CSV File macOS\n\n\n\n\n\n\n\nappetizer\n\n\nimport data\n\n\nmild spicy\n\n\n \n\n\n\n\n2023-03-31\n\n\nRyan Garnett\n\n\n2 min\n\n\n\n\n\n\n\n\nImport CSV File Windows\n\n\n\n\n\n\n\nappetizer\n\n\nimport data\n\n\nmild spicy\n\n\n \n\n\n\n\n2023-03-31\n\n\nRyan Garnett\n\n\n2 min\n\n\n\n\n\n\n\n\nSkip Blank Rows on CSV Import\n\n\n\n\n\n\n\nappetizer\n\n\nimport data\n\n\nmild spicy\n\n\n \n\n\n\n\n2023-03-31\n\n\nRyan Garnett\n\n\n2 min\n\n\n\n\n\n\n\n\nSkip Rows on CSV Import\n\n\n\n\n\n\n\nappetizer\n\n\nimport data\n\n\nmild spicy\n\n\n \n\n\n\n\n2023-03-31\n\n\nRyan Garnett\n\n\n2 min\n\n\n\n\n\n\n\n\nInstall Multiple Packages\n\n\n\n\n\n\n\nappetizer\n\n\ninstalling packages\n\n\nnone spicy\n\n\n \n\n\n\n\n2023-03-29\n\n\nRyan Garnett\n\n\n2 min\n\n\n\n\n\n\n\n\nInstall a Single Package\n\n\n\n\n\n\n\nappetizer\n\n\ninstalling packages\n\n\nnone spicy\n\n\n \n\n\n\n\n2023-03-29\n\n\nRyan Garnett\n\n\n2 min\n\n\n\n\n\n\n\n\nLoad Multiple Packages\n\n\n\n\n\n\n\nappetizer\n\n\nloading packages\n\n\nnone spicy\n\n\n \n\n\n\n\n2023-03-29\n\n\nRyan Garnett\n\n\n2 min\n\n\n\n\n\n\n\n\nLoad Single Package\n\n\n\n\n\n\n\nappetizer\n\n\nloading packages\n\n\nnone spicy\n\n\n \n\n\n\n\n2023-03-29\n\n\nRyan Garnett\n\n\n2 min\n\n\n\n\n\n\nNo matching items"
  }
]