[
  {
    "objectID": "data-stories.html",
    "href": "data-stories.html",
    "title": "Data Chef",
    "section": "",
    "text": "Communicating the findings from a data analysis is fairly standard, however explaining the full data journey to an audience is a relatively new area within the data field; enter Data Stories. The goal of a data story is to share the process, methods, analysis, and findings in a reproducible approach. Data Chef data stories are geared for a wide range of users, but lean on the more technical side, consisting of code snippets (note the code is not intended to be production quality, meaning considerations around optimization, testing, validation, etc. are not the main focus). The focus of these stories are to illustrate approaches for using data and analytics techniques to support informed decisions.\n\n\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nSpeed Up Data Analytics\n\n\n\n\n\n\ndata\n\n\ndata analytics\n\n\ndata wrangling\n\n\nParquet\n\n\n\n\n\n\n2022-04-26\n\n\nRyan Garnett, Ray Wong, Dan Reed\n\n\n2 min\n\n\n\n\n\n\n\nGeospatial Distributed Processing\n\n\n\n\n\n\ndata\n\n\ndistributed processing\n\n\ngeospatial\n\n\n\n\n\n\n2023-01-23\n\n\nRyan Garnett\n\n\n2 min\n\n\n\n\n\n\n\nChoosing an Electric Vehicle\n\n\n\n\n\n\nEV\n\n\nmodelling\n\n\n\n\n\n\n2023-04-14\n\n\nRyan Garnett\n\n\n17 min\n\n\n\n\n\n\n\nDelivering Data Products Faster\n\n\n\n\n\n\ndata\n\n\nHIAA\n\n\nPosit\n\n\nproduct\n\n\n\n\n\n\n2024-01-01\n\n\nRyan Garnett\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/deliver.html",
    "href": "posts/deliver.html",
    "title": "Deliver",
    "section": "",
    "text": "When the meal is complete the last stage in the food journey is delivery. There are different options to getting a meal in the hands of a customer, such as dine-in, take-out, or self-serve (i.e. buffet). The different delivery methods contribute to the overall customer experience, requiring different levels of involvement from customers.\n\nThe data equivalent is information delivery within the area of customer experience. Getting the analysis in the hands of users can take many forms, depending on the findings, and the audience. Choosing a delivery method should be reflective of both the analysis and the user’s needs and abilities. Common data delivery methods are:\n\napplication\ndata service\nemail attachment\nnotification\nscheduled\nself-service\n\n\nKey points\n\nunderstand the audience\nidentify the frequency\nunderstand limitations and challenges"
  },
  {
    "objectID": "posts/access.html",
    "href": "posts/access.html",
    "title": "Access",
    "section": "",
    "text": "The beginning of the cooking journey starts with accessing food. Where the food is accessed from, and the amount, has a connection to both what is being prepared, and who is making the meal. There is a significant difference between a four course meal being prepared at a five star restaurant, and someone looking to make a sandwich for their lunch. These differences are reflective in where the food is accessed from.\n\nThe data equivalent is data access within the area of data engineering. Access to data is an important factor that is typically overlooked. Considerations on who is accessing the data (application or user), what technical abilities they have, and how much data is required is essential for determining the different methods of access, which may include:\n\napplication programming interface (API)\ndirect connection (database)\ndownload (file)\nmanaged service\n\n\nKey points\n\nidentify the type of users\nunderstand user’s technical abilities\ndetermine volume of data needed"
  },
  {
    "objectID": "posts/process.html",
    "href": "posts/process.html",
    "title": "Process",
    "section": "",
    "text": "After being shipped from the farm the plants are evaluated to identify their relative quality, which determines the potential usage. Consider an apple, various factors (i.e. shape, visual appearance, size, number of bruises, etc.) will decide its end usage. That decision will set in motion if the apple should be juiced, pureed into apple sauce, cut and frozen, or used as-is.\n\nThe data equivalent is data transformation within the area of data engineering. In many cases the data collected is not ready for consumption in all use cases. Like with food, data needs to be profiled to identify any defects and determine what transformations (reshape, filter, aggregate, summarize, etc.), what data quality issues (data types, missing values, values out-of-range, etc.), and how to clean identified dirty data.\n\nKey points\n\nperform data profiling\nreview quality of data\nreshape, transform, and clean data"
  },
  {
    "objectID": "posts/grow.html",
    "href": "posts/grow.html",
    "title": "Grow",
    "section": "",
    "text": "The food cycle starts with planting seeds. There is more to growing food the just putting seeds in the ground and letting them grow. There are many factors that are required to be successful many factors. Select the location that as the proper amount of sun, the correct soil type, and the required space is one of the first consideration needed. Once the planting location has been selected the growing area needs to be prepared, by adding extra nutrients to help plants grow within their early stages. While plants are growing they require attention such as watering, weeding, and additional nutrients based on their growth.\n\nThe data equivalent is data collection. The collection of data is the beginning of all data journeys, and like growing plants, there are many things to consider and undertake. Data collection follows five basic questions: what, who, when, where, and how. \nWhat data needs to be collected. For example, user feedback, traffic volumes, environmental conditions, product usage, etc.\nWho is the target “audience” of the data collection\nWhen should the data be collected\nWhere is the optimal location to be collecting the data\nHow will the data be collected, the method or instrument that will be used to collect the data (i.e. survey, interview, sensor, field collection, etc.)\n\nKey points\n\nidentify the audience\nselect the collection method\nstructure the inputs that are being collected"
  },
  {
    "objectID": "posts/plate.html",
    "href": "posts/plate.html",
    "title": "Plate",
    "section": "",
    "text": "When the plants reach the end of their growing season the harvest begins. A harvest can take many forms, depending on the food that is being harvested. A single harvest method will not apply to all food sources, some can use large machinery, while others require delicate manual interaction. Harvest time is also a significant consideration, as certain plants need to be harvested immediately, whereas others have a longer grace period.\n\nThe data equivalent is data presentation within the area of data analytics. There is a tight connection between the analysis, presentation, and delivery of the data. The delivery must be representative of the analysis, while reflecting the available methods of delivery. There are many important elements to consider in order to present data effectively, which are tightly related to the analysis performed and the available delivery method:\n\ndynamic vs. static\ndata vs. dashboard vs. report\nKPI or metric\n\n\nKey points\n\nunderstand the audience and purpose\nensure selected method matches available delivery options\ndetermine the expected level of detail required"
  },
  {
    "objectID": "posts/cook.html",
    "href": "posts/cook.html",
    "title": "Cook",
    "section": "",
    "text": "The most glamorous part of farm to table journey is cooking; it is where the magic happens, the transformation of a raw source into something tasty. There are many different approaches, methods, and considerations that are needed when cooking a meal. Generally a cooking method is chosen (BBQ, bake, boil, fry, etc.) which is incorporated into series of steps outlined in the recipe .\n\nThe data equivalent is data analysis within the area of data analytics. Following the journey from collection, engineering, and wrangling, the data is ready for analysis. There are many different analysis methods available, each with benefits and limitations:\n\nclassification\ndescriptive\ngeospatial\ninferential\nmachine learning / artificial intelligence\npredictive\nprescriptive\n\nSimilar with cooking a meal where the cooking method needs to be paired with the appropriate food and situation, the analysis method used should fit the need of the question to be explored and the available data.\n\nKey points\n\nidentify the question that is to be explored\nselect the method that suits the needs\nensure their is adequate volume of data"
  },
  {
    "objectID": "stories/geospatial-distributed-processing.html",
    "href": "stories/geospatial-distributed-processing.html",
    "title": "Geospatial Distributed Processing",
    "section": "",
    "text": "Everything happens somewhere. Location is an important factor when people make decisions that influences various life decisions, from buying a house, choosing a school, to vacations, or selecting a restaurant to eat out at. Within the data space utilizing location data is becoming increasingly important in data analysis, data science, and machine learning. Data wrangling is an integral part of data science, typically occupying a significant portion of time and effort, with location data adding to the complexity.\nLocation data can be large, consisting of hundreds of thousands to billions of observations in a single dataset. With location-based analysis consistently utilizing multiple datasets, the time to complete the analysis can be lengthy. Similar to other data analytics, the majority of location-based analysis tasks are performed sequentially using a single processor on a computer, even though many of the operations performed have no dependency on the other data or their outcome; if only there were another way…well, let me introduce you to distributed location-based processing.\nWhat is distributed processing? In short, distributed processing is the use of multiple computer processors to execute a single computational task. A distributed process is executed by splitting a task into smaller chunks which are simultaneously performed on multiple processors. We can use distributed processing to make our code run faster, sometimes much faster. For large geospatial datasets that require a lot of computational power, distributed processing can reduce the time needed to handle complex data manipulation.\nIn this post, we’ll explore different techniques for wrangling location data, the processing time required for each, and the significant improvements that can be seen with distributed processing.\nFull story on Posit"
  },
  {
    "objectID": "stories/choosing-ev.html",
    "href": "stories/choosing-ev.html",
    "title": "Choosing an Electric Vehicle",
    "section": "",
    "text": "Electric vehicles (EV) are increasing in popularity, by both the consumer and automakers. There are a significant number of EV available on the market, each having different features and characteristics. With EVs being relatively new to the vehicle market, it can be very difficult to know which EVs to consider as a new vehicle. Within this story we will present a data driven approach for making an informed decision when looking for new electric vehicles. A number of EVs will be evaluated based on objective features, omitting things like appearance, style, etc. A custom analytical model will be created to analyze the features and suggest the top electric vehicles."
  },
  {
    "objectID": "stories/choosing-ev.html#feature-engineering",
    "href": "stories/choosing-ev.html#feature-engineering",
    "title": "Choosing an Electric Vehicle",
    "section": "Feature Engineering",
    "text": "Feature Engineering\nThe format of the data collected on the EVs is not conducive to support the design of the model. Rather than modifying the source data (i.e. changing the values to reflected the desired data format values) new features, or attributes, will be created using the source values.\n\nFeature engineering entails reformatting predictor values to make them easier for a model to use effectively. This includes transformations and encoding of the data to be represent their important characteristics  Tidy Modeling with R  Max Kuhn & Julie Silge\n\nSource data elements were used to created three features for use in the model. New features were encoding using values of 0, 1, 3, 6, and 9 providing a clear distinction between the values of the feature.\n\nPriceRangeDrive TrainEngineered Dataset\n\n\nThe following was used when creating new price features\n\n\n\nPrice Range\nFeature Encoding\n\n\n\n\nLess than $30,000\n9\n\n\n$30,000 to $40,000\n6\n\n\n$40,000 to $50,000\n3\n\n\n$50,000 to $60,000\n1\n\n\nGreater than $60,000\n0\n\n\n\n\n# Function to create new feature\n# Specific to electric vehicle price\ncreate_price_feature &lt;- function(.data, price_column){\n  .data |&gt;\n    dplyr::mutate(price_value = dplyr::case_when(\n    {{price_column}} &lt; 30000 ~ 9,\n    dplyr::between({{price_column}}, 30000, 40000) ~ 6,\n    dplyr::between({{price_column}}, 40000, 50000) ~ 3,\n    dplyr::between({{price_column}}, 50000, 60000) ~ 1,\n    {{price_column}} &gt; 60000 ~ 0\n  ))\n}\n\n\n\nThe following was used when creating new driving range (in kilometres) features\n\n\n\nDriving Range\nFeature Encoding\n\n\n\n\nGreater than 500km\n9\n\n\n400 to 500km\n6\n\n\n300 to 400km\n3\n\n\n200 to 300km\n1\n\n\nLess than 200km\n0\n\n\n\n\n# Function to create new feature\n# Specific to electric vehicle range\ncreate_range_feature &lt;- function(.data, range_column){\n  .data |&gt;\n    dplyr::mutate(range_value = dplyr::case_when(\n    {{range_column}} &gt; 500 ~ 9,\n    dplyr::between({{range_column}}, 400, 500) ~ 6,\n    dplyr::between({{range_column}}, 300, 400) ~ 3,\n    dplyr::between({{range_column}}, 200, 300) ~ 1,\n    {{range_column}} &lt; 200 ~ 0\n  ))\n}\n\n\n\nThe following was used when creating new drive train features\n\n\n\nDrive Train\nFeature Encoding\n\n\n\n\nFour wheel drive (4WD)\n9\n\n\nAll wheel drive (AWD)\n6\n\n\nFront wheel drive (FWD)\n3\n\n\nRear wheel drive (RWD)\n1\n\n\n\n\n# Function to create new feature\n# Specific to electric vehicle drivetrain\ncreate_drivetrain_feature &lt;- function(.data, drivetrain_column){\n  .data |&gt;\n    dplyr::mutate(drivetrain_value = dplyr::case_when(\n    {{drivetrain_column}} == \"4WD\" ~ 9,\n    {{drivetrain_column}} == \"AWD\" ~ 6,\n    {{drivetrain_column}} == \"FWD\" ~ 3,\n    {{drivetrain_column}} == \"RWD\" ~ 1\n  ))\n}\n\n\n\n\nvehicles |&gt;\n  create_drivetrain_feature(drivetrain) |&gt;\n  create_range_feature(range) |&gt;\n  create_price_feature(price) |&gt;\n  DT::datatable()"
  },
  {
    "objectID": "stories/choosing-ev.html#designing-a-model",
    "href": "stories/choosing-ev.html#designing-a-model",
    "title": "Choosing an Electric Vehicle",
    "section": "Designing a Model",
    "text": "Designing a Model\nThe idea behind the model is to combine the three variables into a single value that can be used to objectively suggest which EVs meet the needs of the user. Using this idea, there are two methods to combine the variables, adding or multiplying. For this story two models will be designed, one using an additive approach, and the other using a multiply approach.\nA critical aspect of the model is to have a weighting factor for each of the variables. The thought behind this was that some variables are more important than others.\n\nweights &lt;- list(price = 0.6, range = 0.3, drivetrain = 0.1)\n\nModel Variable Weights\n\n\n\nVariable\nWeight\n\n\n\n\nPrice\n0.6\n\n\nRange\n0.3\n\n\nDrivetrain\n0.1\n\n\n\n\n\nModel One\nThe approach of this model is to apply the variable weight to each variable and add each variable together\n\n\\[\n(price * weightPrice) + (range * weightRange) + (drivetrain * weightDrivetrain)\n\\]\n\n\nBuilding Model OneModel One OutputVisualizing Model One\n\n\n\nmodel1 &lt;- function(.data, manufacture, model, price, range, drivetrain){\n  .data |&gt;\n    dplyr::mutate(model_output = ({{price}} * weights$price) + ({{range}} * weights$range) + ({{drivetrain}} * weights$drivetrain)) |&gt;\n    dplyr::select({{manufacture}}, {{model}}, model_output)\n}\n\n\n\n\n# Applying model one to the vehicle dataset\nvehicles |&gt;\n  model1(manufacture = make,\n         model = model,\n         price = price_value,\n         range = range_value,\n         drivetrain = drivetrain_value)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe top five electric vehicles identified from Model One are Chevy Bolt EV, Hyundai Kona Ultimate, Kia Niro EV, Kia Niro PHEV, Toyota Prius Prime\n \n\n\nModel Two\nThe approach of this model is to apply the variable weight to each variable and multiply each variable together\n\n\\[\n(price * weightPrice) * (range * weightRange) * (drivetrain * weightDrivetrain)\n\\]\n\n\nBuilding Model TwoModel Two OutputVisualizing Model Two\n\n\n\nmodel2 &lt;- function(.data, manufacture, model, price, range, drivetrain){\n  .data |&gt;\n    dplyr::mutate(model_output = ({{price}} * weights$price) * ({{range}} * weights$range) * ({{drivetrain}} * weights$drivetrain)) |&gt;\n    dplyr::select({{manufacture}}, {{model}}, model_output)\n}\n\n\n\n\n# Applying model two to the vehicle dataset\nvehicles |&gt;\n  model2(manufacture = make,\n         model = model,\n         price = price_value,\n         range = range_value,\n         drivetrain = drivetrain_value)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe top five electric vehicles identified from Model Two are Chevy Bolt EV, Hyundai Kona Ultimate, Kia Niro EV, Hyundai Ioniq 5 Preferred AWD Long Range, Nissan Ariya, VW ID.4 Pro AWD"
  },
  {
    "objectID": "data-think.html",
    "href": "data-think.html",
    "title": "Data Chef",
    "section": "",
    "text": "Understanding data and analytics concepts can be challenging, creating a significant barrier for individuals looking to leverage these technologies effectively. This complexity often hinders the ability to make data-driven decisions, limiting the potential benefits that analytics can offer to organizations and individuals alike. Communicating challenging subjects into consumable and understandable concepts is important for building a data culture with individuals who have lower technical understanding.\n\n\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nAnalytical Journey\n\n\n\n\n\n\nanalysis\n\n\nartificial intelligence\n\n\ndescriptive\n\n\npredictive\n\n\nprescriptive\n\n\n\n\n\n\n2024-08-16\n\n\nRyan Garnett\n\n\n5 min\n\n\n\n\n\n\n\nUnderstanding Artifical Intelligence\n\n\n\n\n\n\nartificial intelligence\n\n\nethics\n\n\npolicy\n\n\n\n\n\n\n2024-08-16\n\n\nRyan Garnett\n\n\n8 min\n\n\n\n\n\n\n\nAccuracy vs. Precision\n\n\n\n\n\n\ndata accuracy\n\n\n\n\n\n\n2024-08-16\n\n\nRyan Garnett\n\n\n7 min\n\n\n\n\n\n\n\nMacro Personas\n\n\n\n\n\n\npersonas\n\n\n\n\n\n\n2024-08-16\n\n\nRyan Garnett\n\n\n6 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Data and analytics are becoming important aspects in more organizations, with many making significant investments in the data domain. Utilizing data and analytics can help organizations or any size and from all industries. However, the implementation and transformation toward a data driven organization has tended to focus on needs of large tech-based organizations. Much of the data transformation journey has focused on highly technical areas like big data, real-time data streaming, machine learning, and artificial intelligence. While extremely important, powerful, and important, not all organizations require the bleeding edge of data analytics. This over emphasis has made it very difficult for smaller organizations, or those early in their data maturity to implement data analytics into their organizational culture.\n\nData Chef is looking to help by demystifying the data and analytics domain through knowledge sharing with data content designed for a wide range of users, not just those with high technical abilities and understanding. A wide range of content covering data literacy, data governance, data management, and analytics, for all…not just large tech organizations.\n\nIf you have data needs reach out via email"
  },
  {
    "objectID": "recipes/accuracy-vs-precision.html",
    "href": "recipes/accuracy-vs-precision.html",
    "title": "Accuracy vs. Precision",
    "section": "",
    "text": "Accuracy and precision are two methods that are used to measure data. The two terms are misunderstood and commonly used incorrectly.\n\nAccuracy: how correct something is to the correct or known value. It is possible to have high accuracy and low precision\nPrecision: how close two or more elements are to each other. It is possible to have high precision and low accuracy"
  },
  {
    "objectID": "recipes/accuracy-vs-precision.html#date-data",
    "href": "recipes/accuracy-vs-precision.html#date-data",
    "title": "Accuracy vs. Precision",
    "section": "Date Data",
    "text": "Date Data\nDates and time can be expressed using a number of different elements; year, month, week, day, hours, minutes, and seconds. The example below illustrates different ways of representing dates and time, expressing now with different degrees of accuracy, moving from year, month, week, and day, to dates with hours, minutes, and seconds.\n\n\n\n\n\n\n\n\n\n\n\n\n\nVery Low\n\n\n\n\n\nVery High\n\n\n\n\n2024-01-01\n2024-08-01\n2024-08-18\n2024-08-21\n2024-08-21 10:00:00\n2024-08-21 10:28:00\n2024-08-21 10:28:30"
  },
  {
    "objectID": "recipes/accuracy-vs-precision.html#number-data",
    "href": "recipes/accuracy-vs-precision.html#number-data",
    "title": "Accuracy vs. Precision",
    "section": "Number Data",
    "text": "Number Data\nThe location of the Halifax International Airport Authority can be expressed in a few different ways, but lets use latitude/longitude coordinates (map location) as an example. Coordinate locations are represented as a set of two numbers, typically including decimal values. The more decimal values provided can increase the accuracy and precision of the location. The example below illustrates different ways of representing location coordinates for the Halifax International Airport Authority, with very low to very high levels of accuracy/precision.\n\n\n\n\n\n\n\n\n\n\n\n\nVery Low\n\n\n\n\nVery High\n\n\n\n\n-64, 44\n-63.5, 44.9\n-63.51, 44.88\n-63.509, 44.881\n-63.5086, 44.8811\n-63.50861, 44.88111"
  },
  {
    "objectID": "recipes/about-ai.html",
    "href": "recipes/about-ai.html",
    "title": "Understanding Artifical Intelligence",
    "section": "",
    "text": "Artificial intelligence (AI) refers to the development of computer systems that can perform tasks typically requiring human intelligence. These tasks include learning from experience, recognizing patterns, making decisions, and solving problems. AI systems are designed to analyze vast amounts of data, extract meaningful insights, and apply them to various domains such as healthcare, finance, transportation, and more. They utilize algorithms and computational models to simulate cognitive functions like perception, reasoning, and decision-making. AI technology continues to evolve rapidly, driving innovation across industries and reshaping the way we interact with machines and process information."
  },
  {
    "objectID": "recipes/about-ai.html#how-large-language-models-and-gpt-work",
    "href": "recipes/about-ai.html#how-large-language-models-and-gpt-work",
    "title": "Understanding Artifical Intelligence",
    "section": "How Large Language Models and GPT Work",
    "text": "How Large Language Models and GPT Work\nOpenAI’s ChatGPT 4.o is a sophisticated language model designed to mimic human conversation. It’s built on advanced technology called the Transformer architecture, which enables it to understand the context and meaning behind words in sentences. Prior to engaging in conversations, the model undergoes extensive training on a vast dataset composed of various texts, such as books and online content. Through this training process, it learns to generate coherent and contextually relevant responses. Additionally, it can be fine-tuned for specific tasks to further enhance its performance. Its capabilities extend beyond simple interactions, as it can be utilized for tasks like chatbot development and language translation.\n\n\nLimitations of Large Language Models and GPT\nWhile ChatGPT is an impressive tool, users should be aware of some limitations:\n\n\n\n\n\n\n\nLimitation\nDescription\n\n\n\n\nLack of Understanding\nChatGPT doesn’t truly understand the context or meaning of what it’s saying. It generates responses based on patterns it learned during training, which can lead to occasional inaccuracies or nonsensical outputs.\n\n\nBias and Offensive Content\nLike any model trained on internet data, ChatGPT may inadvertently produce biased or offensive responses. It’s important for users to monitor and filter its outputs, especially in sensitive or public-facing applications.\n\n\nInability to Handle Complex Tasks\nChatGPT may struggle with tasks that require deep understanding, reasoning, or domain-specific knowledge. It’s best suited for simpler, text-based interactions and may not be suitable for tasks requiring complex decision-making.\n\n\nPrivacy Concerns\nSharing sensitive or personal information with ChatGPT could pose privacy risks, as it may retain and learn from the data provided during interactions. Users should exercise caution when discussing confidential or sensitive topics.\n\n\nLimited Creativity and Originality\nWhile ChatGPT can generate diverse responses, it lacks true creativity and originality. Responses may sometimes feel repetitive or formulaic, especially when generating longer pieces of text.\n\n\nReliance on Prompt Quality\nThe quality of responses generated by ChatGPT heavily depends on the quality and clarity of the prompts provided by the user. Ambiguous or poorly worded prompts may result in less relevant or coherent responses."
  },
  {
    "objectID": "recipes/about-ai.html#developing-an-artificial-intelligence-ethics-and-policy",
    "href": "recipes/about-ai.html#developing-an-artificial-intelligence-ethics-and-policy",
    "title": "Understanding Artifical Intelligence",
    "section": "Developing an Artificial Intelligence Ethics and Policy",
    "text": "Developing an Artificial Intelligence Ethics and Policy\nAn AI policy and ethics framework is crucial for organizations to navigate the opportunities and challenges associated with AI technologies responsibly and ethically, while also maximizing the benefits for all stakeholders involved.\n\n\n\n\n\n\n\nReason\nDescription\n\n\n\n\nGuidance and Standards\nAI policies provide organizations with clear guidance and standards on how AI technologies should be developed, deployed, and used within the organization. This helps ensure consistency, accountability, and compliance with legal and regulatory requirements.\n\n\nRisk Management\nEstablishing AI ethics helps organizations identify and mitigate potential risks associated with AI technologies, such as bias, discrimination, privacy violations, and unintended consequences. By addressing these risks proactively, organizations can minimize negative impacts and protect their reputation.\n\n\nTrust and Transparency\nA well-defined AI policy promotes trust and transparency among stakeholders, including customers, employees, investors, and regulators. It demonstrates the organization’s commitment to responsible AI practices and ethical principles, fostering confidence in the organization’s AI initiatives.\n\n\nLegal Compliance\nAI policies ensure that organizations comply with relevant laws, regulations, and industry standards related to AI technologies. This reduces the risk of legal liabilities and penalties associated with non-compliance, such as fines, lawsuits, and damage to the organization’s reputation.\n\n\nEthical Considerations\nEthical AI policies help organizations address complex ethical considerations associated with AI technologies, such as fairness, accountability, transparency, privacy, and the impact on society. By integrating ethical principles into their AI strategies, organizations can align their actions with societal values and contribute to positive social outcomes.\n\n\nCompetitive Advantage\nAdopting ethical AI practices can provide organizations with a competitive advantage by differentiating them from competitors who may lack clear policies or engage in unethical AI practices. Ethical AI can enhance brand reputation, attract customers who value responsible AI, and attract top talent who want to work for socially responsible organizations.\n\n\nLong-Term Sustainability\nImplementing AI policies and ethics fosters long-term sustainability by promoting responsible use of AI technologies that consider the interests of all stakeholders, including present and future generations. This helps organizations build trust and goodwill over time, positioning them for continued success in an increasingly AI-driven world."
  },
  {
    "objectID": "recipes/analytics-evolution.html",
    "href": "recipes/analytics-evolution.html",
    "title": "Analytical Journey",
    "section": "",
    "text": "In the world of data analytics there are many different analytical methods available. Understanding what can be done with the different methods can be challenging and confusing. An analytics journey within an organization typically starts with with historical reporting, with long term aspirations of moving into machine learning and artificial intelligence.\nBetween the two endpoints, reporting and artificial intelligence, are a number of different options. Moving along the analytics journey the methods increase in complexity.\nBasic\n\n\nIntermediate\n\n\nAdvance\n\n\n\nReporting\nInference\nProbability\nClustering\nPrediction\nPrescriptive\nArtifical Intelligence\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat happened\nIs what happened significant\nChances of something happening\nWhat is similar\nWhat could happen\nHow to take action\nContinually evolving"
  },
  {
    "objectID": "recipes/analytics-evolution.html#what-happened",
    "href": "recipes/analytics-evolution.html#what-happened",
    "title": "Analytical Journey",
    "section": "What Happened",
    "text": "What Happened\n\nThe use of this method is retrospective, providing insight on things that have happened. Examples of what happened are:\n\nHow many products were sold last month\nWhat is the average number of visitors on a Friday\nWhat is the minimum amount of time spent visiting a site"
  },
  {
    "objectID": "recipes/analytics-evolution.html#is-what-happened-significant",
    "href": "recipes/analytics-evolution.html#is-what-happened-significant",
    "title": "Analytical Journey",
    "section": "Is What Happened Significant",
    "text": "Is What Happened Significant\n\nThe use of this method is to make estimates of a population, or drawing conclusion of a hypothesis. Examples of is what happened significant are:\n\nWas yesterdays usage normal\nIs the increase in energy usage an outlier\nWas the amount of precipitation last month an anomaly"
  },
  {
    "objectID": "recipes/analytics-evolution.html#chances-of-something-happening",
    "href": "recipes/analytics-evolution.html#chances-of-something-happening",
    "title": "Analytical Journey",
    "section": "Chances of Something Happening",
    "text": "Chances of Something Happening\n\nThe use of this method is to estimate the likelihood of an event happening. Examples of chances of something happening are:\n\nIf a weather event will cause a travel delay\nPossibility of a customer making a return\nLikelihood of client using a new service"
  },
  {
    "objectID": "recipes/analytics-evolution.html#what-is-similar",
    "href": "recipes/analytics-evolution.html#what-is-similar",
    "title": "Analytical Journey",
    "section": "What is Similar",
    "text": "What is Similar\n\nThe use of this method is to identify groups that share similar properties. Examples of what is similar are:\n\nCreate customer profile segmentation\nClassify travel delays\nIdentify hotspots where health outbreaks occurred"
  },
  {
    "objectID": "recipes/analytics-evolution.html#what-could-happen",
    "href": "recipes/analytics-evolution.html#what-could-happen",
    "title": "Analytical Journey",
    "section": "What Could Happen",
    "text": "What Could Happen\n\nThe use of this method is to forecast potential future outcomes. Examples of what could happen are:\n\nEstimate wait time under different conditions\nForecast new service usage\nScenario planning"
  },
  {
    "objectID": "recipes/analytics-evolution.html#how-to-take-action",
    "href": "recipes/analytics-evolution.html#how-to-take-action",
    "title": "Analytical Journey",
    "section": "How to Take Action",
    "text": "How to Take Action\n\nThe use of this method is to leverage past outcomes and trends to identify what needs to completed to achieve future goals. Examples of how to take action are:\n\nRecommend optimal service plan\nPrioritizing work orders with the least operational impact\nSelection of products with highest revenue potential"
  },
  {
    "objectID": "recipes/analytics-evolution.html#continually-evolving",
    "href": "recipes/analytics-evolution.html#continually-evolving",
    "title": "Analytical Journey",
    "section": "Continually Evolving",
    "text": "Continually Evolving\n\nThe use of this method is to identify unseen outcomes. Examples of continually evolving are:\n\nCustomer chatbot\nDemand based pricing\nFuture based evaluation of work schedules"
  },
  {
    "objectID": "recipes/macro-personas.html",
    "href": "recipes/macro-personas.html",
    "title": "Macro Personas",
    "section": "",
    "text": "Informed decisions can be made by many different people within an organization, either directly or indirectly. A decision can be made directly from a source, or through a chain of interactions and users, each having their own perspective, purpose, needs, and abilities.\nLeveraging philosophies from different areas within the tech sector, specifically human centric design and design thinking, can help with designing data products that meet the needs of a wide range of users. A common approach is to use persons when building products, as said by Anthony Salerno:\n\nPersonas depict the attitudes, behaviours, and motivations of customers. Unlike traditional marketing segments which focus on demographic attributes, personas focus on how customers think and why they do or think the way they do\n\nFollowing that idea of personas, users within most organizations can fit into the following five personas.\n\n\n\n\n\n\n\n\n\n\nPersona\nDescription\n\n\n\n\n\nAnalyst\nPerforms analysis and various data related processing tasks  Work performed is very technical in nature\n\n\n\nApplication\nSoftware application allowing users to perform a series of tasks  Work performed can be technical or non-technical in nature\n\n\n\nDecision Maker\nSets directions and makes recommendations on various elements related to an organization’s future direction  Work performed is very communication focused\n\n\n\nIntegrator\nEnables users to access information by connecting systems together  Worked performed is very technical in nature\n\n\n\nOperator\nEngages with people to help provide critical information through interactions with various organizations systems  Work performed is very customer focused\n\n\n\n\nKnowing that users have different levels of technical understanding and ability, products should be designed to meet a range of technical ability; from low to high. Designed and developed products according to the target audience and their respected technical ability following three levels:\n\n\n\n\n\n\n\nLow\nUser interface: Yes with minimal inputs and interactions (i.e. Excel)  Code: No but comfortable using functions with spreadsheets (i.e. Excel)\n\n\nModerate\nUser interface: Yes with ability to interact with data and perform analysis (i.e. Tableau)  Code: Yes with little emphasis on automation and optimization (i.e. Excel)\n\n\nHigh\nUser interface: No with minimal inputs and interactions (i.e. code base functions)  Code: Yes (i.e. R, Python, etc.)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Chef",
    "section": "",
    "text": "Serving data knowledge for everyone through content and strategy"
  },
  {
    "objectID": "stories/delivering-products-faster.html",
    "href": "stories/delivering-products-faster.html",
    "title": "Delivering Data Products Faster",
    "section": "",
    "text": "Posit Connect has empowered the Halifax International Airport Authority (HIAA) to deliver products following a rapid prototyping methodology, putting versions of a product in the hands of users in days – allowing for actionable feedback and updates that are back to users in hours.\nIn a few short months after implementing Posit Connect, their team has achieved numerous successes. The range of data-driven content available to users expanded from just two dashboards to include:\n\nInteractive applications (e.g. flight schedule search)\nDynamic dashboards (e.g. passenger volume, YHZ Express utilization)\nA knowledge centre focused on data analytics learning materials\n\nPosit Connect, being built with open-source architectures in mind means that we can easily integrate it with our data platform, allowing users to interact with HIAA Data Analytics Hub as a store front for accessing analytical content and data artifacts, opening the door to a wider range of users. This code-first approach, combined with the extensive available R libraries, also allows for integrations with Microsoft Teams, our organization’s internal communication platform.\nFull Story on Posit"
  },
  {
    "objectID": "stories/speed-up-analytcs.html",
    "href": "stories/speed-up-analytcs.html",
    "title": "Speed Up Data Analytics",
    "section": "",
    "text": "Data is increasing in value for many organizations — an asset leveraged to help make informed business decisions. Unfortunately, this sentiment has not been the norm throughout the life of most organizations, with vast amounts of data locked in legacy data management systems and designs. The majority of organizations use relational database management systems (RDBMS) like Oracle, Postgres, Microsoft SQL, or MySQL to store and manage their enterprise data. Typically these systems were designed to collect and process data quickly within a transactional data model. While these models are excellent for applications, they can pose challenges for performing business intelligence, data analytics, or predictive analysis. Many organizations are realizing their legacy systems are not sufficient for data analytics initiatives, providing an opportunity for analytics teams to present tangible options to improve their organization’s data analytics infrastructure.\nRegardless if you are engineering data for others to consume for analysis, or performing the analytics, reducing the time to perform data processing is critically important. Within this post, we are going to evaluate the performance of two distinct data storage formats; row-based (CSV) and columnar (parquet); with CSV being a tried and tested standard data format used within the data analytics field, and parquet becoming a viable alternative in many data platforms.\nFull story on Posit"
  },
  {
    "objectID": "farm-to-table.html",
    "href": "farm-to-table.html",
    "title": "Data Chef",
    "section": "",
    "text": "Data is becoming increasingly important to more organizations, however understanding the lifecycle and all the different aspects that are associated to the data space can be challenging. Data literacy is an important, under appreciated, component of a data organization. The concept of farm to table is used to explain the stages and progression within a data related initiative.\n\nFarm to table  Noting or relating to the stages involved in the growing, processing, and consumption of food\n\n\n\n\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nGrow\n\n\n\n\n\n\nfarming\n\n\ndata collection\n\n\n\n\n\n\n2023-02-04\n\n\nRyan Garnett\n\n\n2 min\n\n\n\n\n\n\n\nHarvest\n\n\n\n\n\n\nfarming\n\n\ndata engineering\n\n\n\n\n\n\n2023-02-05\n\n\nRyan Garnett\n\n\n1 min\n\n\n\n\n\n\n\nShip\n\n\n\n\n\n\nsupply chain\n\n\ndata engineering\n\n\n\n\n\n\n2023-02-06\n\n\nRyan Garnett\n\n\n1 min\n\n\n\n\n\n\n\nProcess\n\n\n\n\n\n\nsupply chain\n\n\ndata engineering\n\n\n\n\n\n\n2023-02-07\n\n\nRyan Garnett\n\n\n1 min\n\n\n\n\n\n\n\nPackage\n\n\n\n\n\n\nsupply chain\n\n\ndata engineering\n\n\n\n\n\n\n2023-02-08\n\n\nRyan Garnett\n\n\n1 min\n\n\n\n\n\n\n\nStore\n\n\n\n\n\n\nsupply chain\n\n\ndata engineering\n\n\n\n\n\n\n2023-02-09\n\n\nRyan Garnett\n\n\n1 min\n\n\n\n\n\n\n\nAccess\n\n\n\n\n\n\ncooking\n\n\ndata engineering\n\n\n\n\n\n\n2023-02-10\n\n\nRyan Garnett\n\n\n1 min\n\n\n\n\n\n\n\nPrepare\n\n\n\n\n\n\ncooking\n\n\ndata wrangling\n\n\n\n\n\n\n2023-02-11\n\n\nRyan Garnett\n\n\n1 min\n\n\n\n\n\n\n\nCook\n\n\n\n\n\n\ncooking\n\n\ndata analytics\n\n\n\n\n\n\n2023-02-12\n\n\nRyan Garnett\n\n\n1 min\n\n\n\n\n\n\n\nPlate\n\n\n\n\n\n\ncooking\n\n\ndata analytics\n\n\n\n\n\n\n2023-02-13\n\n\nRyan Garnett\n\n\n1 min\n\n\n\n\n\n\n\nDeliver\n\n\n\n\n\n\ncustomer experience\n\n\ninformation delivery\n\n\n\n\n\n\n2023-02-14\n\n\nRyan Garnett\n\n\n1 min\n\n\n\n\n\n\n\nEat\n\n\n\n\n\n\ncustomer experience\n\n\ninformed decision\n\n\n\n\n\n\n2023-02-15\n\n\nRyan Garnett\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/prepare.html",
    "href": "posts/prepare.html",
    "title": "Prepare",
    "section": "",
    "text": "Prior to cooking most food require some level of preparation, such as being pealed, copped, sliced, mixed, or marinaded. The preparation may include phases where multiple ingredients are combined together before the cooking portion begins. The steps taken will depend on the recipe that is being cooked, as not all meals require the same type, or level, of preparation.\n\nThe data equivalent is data wranging. The extraction of data is the starting point of data pipelines. Similar to harvesting plants there are considerations when extracting data, such as what data is to be extracted (all tables from a source, specific columns), when the extraction should occur (specific time, reoccurring time, ad hoc), how the data should be extracted (manual vs. automated), and where is the data being extracted from (structured/unstructured data, database, file, service, etc.). Understanding these components will assist with developing a data extraction method.\n\nKey points\n\nselect the extraction method that fits the data source\ndesign for the data source\ndetermine the intended usage pattern"
  },
  {
    "objectID": "posts/eat.html",
    "href": "posts/eat.html",
    "title": "Eat",
    "section": "",
    "text": "It is time to enjoy the meal. Eating food comes in many forms, from a snack, quick lunch, family supper, to a large multi course special occasion, with each having a specific purpose and people attending. The reason for eating has an influence on number of people, the frequency, and re-occurrence of the meal.\n\nThe data equivalent is informed decision within the area of customer experience. The use of data to help inform an objective decision is important for many organizations. There are a number of factors and considerations when making an informed decision:\n\nfrequency of the decision\n\nad hoc decision\nre-occurring decision\n\naudience\n\nindividual decision maker\ngroup decision\n\nsize and scope of the decision\nresult and outcome\n\ndecision is final\ndecision leads to additional analysis\n\n\n\nKey points\n\nunderstand the magnitude of the decision\nidentify the audience\nidentify frequency and re-occurrence of decision"
  },
  {
    "objectID": "posts/package.html",
    "href": "posts/package.html",
    "title": "Package",
    "section": "",
    "text": "After the plants have undergone processing they are put into an appropriate package to assist with the next stages of the journey. The packaging is dependent on the type of processing that was performed. Reconsidering an the apple from processing, different containers will be chosen for apple juice than that of apple sauce, frozen apple slices, or apples that will be sold as fresh produce.\n\nThe data equivalent is data model within the area of data engineering. Like with food data is better consumed when the packaging matches it’s intended use. When creating a data model considerations of how the data will be used (i.e. record management, analytics, relationships, etc.) will influence what data model (transactional, analytical, graph, relational) is required. Many data usage cases cannot be resolved with a single data model, meaning multiple data models may be needed.\n\nKey points\n\ndetermine all potential uses of the data\nidentify the appropriate data model(s)\ndesign the data model structure and elements"
  },
  {
    "objectID": "posts/ship.html",
    "href": "posts/ship.html",
    "title": "Ship",
    "section": "",
    "text": "Following the harvest the plants enter a new journey and are moved from the farm. The plants are bundled and placed into a vehicle to be transported to the next stage. The method and length of the transport depends on the crop, as some require refrigeration, while others do not.\n\nThe data equivalent is data transit within the area of data engineering. The purpose of data transit is to move the data from one stage to another. Like with shipping food considerations on how the data is moved (streaming vs. file transfer), and when the data needs to be moved (constantly, or in pieces) is needed in order to build a data pipeline that meets the requirements of the next stage and the users.\n\nKey points\n\nchoose the method that matches the data\nidentify the volume of data to be moved\nunderstand the speed needed to move the data"
  },
  {
    "objectID": "posts/store.html",
    "href": "posts/store.html",
    "title": "Store",
    "section": "",
    "text": "The final stage in a food’s supply chain journey is storage. A number of options are available (i.e. market, grocery store, warehouse, etc.) each designed and suited for different food choices. Following the example of the apple from previous posts, a bag of frozen apple slices would be better suited for a grocery store or warehouse, rather than a farmer’s market.\n\nThe data equivalent is data storage within the area of data engineering. Selecting a data storage system that is not inline with the intended use case can introduce significant limitations and challenges. When deciding on a data storage system there are a number of factors to consider, such as:\n\ncloud or on-premise or hybrid\nenterprise or local\nstructured or unstructured\ndatabase or file\nrow or columnar\n\n\nKey points\n\nunderstand the type of use\nunderstand the usage volume\nunderstand the volume of data"
  },
  {
    "objectID": "posts/harvest.html",
    "href": "posts/harvest.html",
    "title": "Harvest",
    "section": "",
    "text": "When the plants reach the end of their growing season the harvest begins. A harvest can take many forms, depending on the food that is being harvested. A single harvest method will not apply to all food sources, some can use large machinery, while others require delicate manual interaction. Harvest time is also a significant consideration, as certain plants need to be harvested immediately, whereas others have a longer grace period.\n\nThe data equivalent is data extraction within the area of data engineering. The extraction of data is the starting point of data pipelines. Similar to harvesting plants there are considerations when extracting data, such as what data is to be extracted (all tables from a source, specific columns), when the extraction should occur (specific time, reoccurring time, ad hoc), how the data should be extracted (manual vs. automated), and where is the data being extracted from (structured/unstructured data, database, file, service, etc.). Understanding these components will assist with developing a data extraction method.\n\nKey points\n\nselect the extraction method that fits the data source\ndesign for the data source\ndetermine the intended usage pattern"
  }
]